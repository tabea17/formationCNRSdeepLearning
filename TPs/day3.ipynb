{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0fb83e-2bbc-4a2a-9462-0b35fdae56a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# TP: Introduction au deep learning avec `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fd7be6-8d64-46c1-9334-de63c7712455",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "`PyTorch` est un framework d'apprentissage automatique open source développé principalement par Facebook's AI Research Lab. Il est basé sur le langage de programmation Python et est connu pour sa flexibilité et sa facilité d'utilisation. `PyTorch` est largement utilisé pour la création et l'entraînement de réseaux de neurones, en particulier dans les domaines de la vision par ordinateur et du traitement du langage naturel.\n",
    "\n",
    "Le coeur de `PyTorch` est écrit en `C++` pour des raisons de performances. Il utilise des bibliothèques mathématiques et d'algèbre linéaire bien établies, telles que `BLAS` (Basic Linear Algebra Subprograms) et `LAPACK` (Linear Algebra Package), pour accélérer les calculs numériques.\n",
    "\n",
    "`PyTorch` fournit une interface Python conviviale pour interagir avec les fonctionnalités de bas niveau. Il utilise une approche orientée objet, ce qui signifie que les différentes fonctionnalités sont organisées en classes et objets. Les utilisateurs peuvent créer des instances de ces objets pour construire des modèles, définir des couches de réseau, définir des fonctions de perte, etc.\n",
    "\n",
    "La bibliothèque `PyTorch` comprend également des modules pour le calcul automatique des gradients (**différentiation automatique**), qui est une fonctionnalité essentielle pour l'apprentissage automatique. Ces modules permettent de calculer automatiquement les dérivées des opérations effectuées sur les tenseurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8edead8-38eb-46bf-8909-efe16c1cdece",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Jeu de données MNIST\n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology) est une base de données largement utilisée dans le domaine de l'apprentissage automatique pour la reconnaissance de chiffres manuscrits. Il s'agit d'un ensemble de données contenant des images en niveaux de gris de chiffres manuscrits de 0 à 9.\n",
    "\n",
    "La base de données MNIST est composée de deux parties principales :\n",
    "- Le jeu d'apprentissage (training set) : Il comprend $60\\,000$ exemples d'images de chiffres manuscrits, chacune étant une image de 28x28 pixels. Ces images sont associées à des étiquettes (labels) qui indiquent le chiffre correspondant (de 0 à 9).\n",
    "- Le jeu de test (test set) : Il comprend $10\\,000$ exemples supplémentaires d'images de chiffres manuscrits, également de taille 28x28 pixels. Les images de test sont utilisées pour évaluer les performances des modèles d'apprentissage automatique entraînés sur le jeu d'apprentissage.\n",
    "\n",
    "La base de données MNIST est souvent utilisée comme un point de départ pour les tâches de classification d'images et pour l'évaluation des algorithmes d'apprentissage automatique. De nombreux chercheurs et développeurs utilisent MNIST pour tester de nouvelles architectures de réseaux de neurones et de nouvelles techniques d'apprentissage.\n",
    "\n",
    "En raison de sa simplicité et de sa taille relativement petite, MNIST est devenu un jeu de données standard pour la communauté de l'apprentissage automatique. Il est souvent utilisé pour illustrer des concepts tels que la préparation des données, la classification, la rétropropagation, l'optimisation et la régularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef5b9e-2c41-43b9-a166-3dcf7a1a032e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22bf96-9c70-4a28-a0d5-aecda59552ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Chargement des données\n",
    "\n",
    "Le module `torchvision.datasets` est une composante du module `PyTorch` spécialisée dans le chargement et la gestion de jeux de données populaires: MNIST, CIFAR-10, ImageNet, etc.\n",
    "\n",
    "La classe principale dans `torchvision.datasets` est Dataset, qui définit l'interface de base pour les jeux de données. Les classes spécifiques à chaque jeu de données héritent de cette classe et la personnalisent en fonction des caractéristiques spécifiques du jeu de données. Par exemple, `MNIST` est une sous-classe de `Dataset` et voici un appel standard: \n",
    "```python\n",
    "from torchvision import datasets\n",
    "mnist_train = datasets.MNIST('data', train=True, download=True)\n",
    "```\n",
    "Une fois l'instance du jeu de données créée, vous pouvez accéder aux exemples de données et à leurs étiquettes à l'aide d'indexing (opérateur `[]`).\n",
    "\n",
    "- Créer un objet `mnist_train` de la classe `datasets.MNIST` (chargement des données).\n",
    "- Déterminer le type des éléments puis afficher quelques images en indiquant les labels associés.\n",
    "- Faire de même pour le dataset de test: `mnist_test`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90872e11-5df9-4243-b896-720e02ba19c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Transformation des images \n",
    "\n",
    "- A partir de `mnist_train` et des champs `data` et `targets`, créer un tensor `x_train` de shape 1x28x28 dont les données sont normalisées dans $[0,1]$ (0 pour un pixel blanc et 1 pour un pixel noir, il faut donc diviser par 255) et un tensor `y_train`. La première dimension (qui semble inutile) est nécessaire car les images en `PyTorch` sont manipulables sous le format `[channels, height, width]`, une image en couleur a 3 canaux R-G-B.\n",
    "- Faire de même un `x_test` et `y_test` à partir de `mnist_test`.\n",
    "\n",
    "**Remarque**:\n",
    "Le module `transforms` du package `torchvision` fournit une collection de transformations couramment utilisées pour prétraiter les images de manière à les adapter aux besoins spécifiques de l'apprentissage automatique, tels que le redimensionnement, le recadrage, la normalisation, etc. \n",
    "On peut utiliser la fonction `ToTensor` pour convertir une image du dataset `mnist_train` en un tensor `PyTorch` et faire un appel de `datasets.MNIST` avec l'option `transform` pour obtenir directement les données sous la forme de tensors. \n",
    "\n",
    "Dans la suite on travaille avec ces données `x_train`, `y_train`, `x_test` et `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba89ba-cf7b-4675-9857-380b217c3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0657a8c-d4d1-4f34-b943-0641311c0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbda13d-a04e-4383-af84-b7f045073c23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Question: Exploration du dataset  \n",
    "\n",
    "- Parcourir les deux datasets pour déterminer la proportion des différents labels présents. Avant toute modélisation il est important de voir si les données sont homogènes: c'est à dire correctement réparties dans le train set et dans le test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efbf89a-d515-4e72-8970-361659e8c591",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Un premier modèle: régression softmax\n",
    "\n",
    "Dans cette première partie, nous n'allons pas utiliser le fait que nos données $x$ sont des images. En effet, chaque image $x$ est une matrice $p\\times p = 28\\times 28$ ($x=(x_{ij})$) avec une structure de voisinage spécifique. Mais ici, nous traitons chaque image $x$ comme un vecteur de taille $784$ ($x = (x_{j})$) et nous ignorons la structure de voisinage.\n",
    "\n",
    "Nous souhaitons classer ces images ou, de manière équivalente, prédire le chiffre $k$ variant dans ${0, \\ldots, 9}$ qu'elles représentent.\n",
    "Un modèle simple permettant de le faire est la régression softmax (ou régression logistique multinomiale).\n",
    "\n",
    "\n",
    "L'idée est de produire un score pour chaque image d'entrée $x$ en utilisant un modèle linéaire simple.\n",
    "Pour cela, nous supposons que l'appartenance à une classe $k$ (correspondant au chiffre $k$) peut être exprimée par une somme pondérée des intensités de pixels, avec des poids $W_{k, 1}, \\ldots, W_{k, 784}$ et un biais (ou intercept) $b_k$ qui capture une variabilité indépendante de l'entrée :\n",
    "$$\n",
    "    \\text{score}_k(x) = \\sum_{j=1}^{784} W_{k, j} x_j + b_k,\n",
    "$$\n",
    "Ces scores sont parfois appelés \"logits\" dans la communauté de l'apprentissage profond.\n",
    "Ensuite, nous utilisons la fonction softmax pour convertir les scores en probabilités prédites $p_k=\\mathbb{P}(y=k|x)$ :\n",
    "$$\n",
    "    \\forall k =0,\\ldots,9,\\quad p_k(x) = \\text{softmax}(\\text{score}_k(x)) = \\frac{\\exp(\\text{score}_k(x))}{\\sum_{\\ell =0}^{9}\\exp(\\text{score}_{\\ell}(x))}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d48e4c1-e4ae-4ca2-b876-0554fe8ef5ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Module `torch.nn`\n",
    "\n",
    "`torch.nn` est un module de `PyTorch` qui fournit des outils et des classes pour construire et entraîner des réseaux de neurones. Il fournit des blocs de construction pour définir les différentes couches et opérations nécessaires dans un réseau de neurones, ainsi que des fonctions d'activation, des fonctions de coût et d'autres fonctionnalités liées à l'apprentissage automatique.\n",
    "\n",
    "Voici quelques éléments clés de `torch.nn`:\n",
    "\n",
    "- Modules et couches: variété de modules et de couches pré-définis tels que `Linear`, `Conv2d`, `RNN`, `BatchNorm`, etc. Ces modules encapsulent des opérations spécifiques et sont utilisés pour construire des architectures de réseaux de neurones complexes.\n",
    "\n",
    "- Fonctions d'activation: `ReLU`, `Sigmoid`, `Tanh`, etc., qui peuvent être appliquées aux sorties des couches pour introduire des non-linéarités dans le modèle.\n",
    "\n",
    "- Fonctions de coût: `CrossEntropyLoss`, `MSELoss`, etc., qui sont utilisées pour évaluer la performance du modèle et guider l'apprentissage.\n",
    "\n",
    "- Optimiseurs: `SGD`, `Adam`, `RMSprop`, etc., qui sont utilisés pour ajuster les poids du modèle pendant l'entraînement.\n",
    "\n",
    "- Définition des réseaux de neurones personnalisés: on peut définir des réseaux de neurones personnalisés en créant des classes héritant de `torch.nn.Module`. Cela permet de définir des architectures complexes en combinant différentes couches et en définissant la logique de propagation avant (forward pass) à l'intérieur de la classe.\n",
    "\n",
    "Pour créer rapidement et facilement des architectures de réseaux de neurones séquentiels, on peut utiliser le module `nn.Sequential`: il suffit de définir l'architecture d'un modèle en spécifiant simplement les couches dans l'ordre dans lequel elles doivent être appliquées. Cela permet de définit un modèle simple sans écrire de classe personnalisée héritant de `nn.Module`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5063724-fbef-4a61-9694-7c631dd4483f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Définition du modèle \n",
    "\n",
    "- En utilisant les modules `nn.Sequential`, `nn.Flatten` et `nn.Linear` définir un objet `model_linear` qui code la fonction $\\text{score}_k: [0,1]^{784} \\to \\mathbf{R}^{10}$. En fait cette fonction sera appelée pour un ensemble d'images..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383ba17-1c54-4f8c-949e-0313e6231e2a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Accès aux paramètres du modèle\n",
    "\n",
    "- Utiliser la méthode `parameters()` sur l'objet `model_linear` et sur les objets `model_linear[i]` (i=0,1) pour accéder aux paramètres des différentes couches.\n",
    "\n",
    "- Sauver les paramètres (l'état) du modèle dans un dictionnaire `state_init`. On utilisera la méthode `state_dict()` et on pourra recharger cet état initial via la méthode `load_state_dict()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07ce53-0846-4120-ae27-9f56212c35fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Fonction de perte et descente de gradient\n",
    "\n",
    "Pour entraîner les paramètres du modèle (le biais $b_k$ et les poids $W_{k, j}$ où $k=0, \\ldots, 9$ et $j=1, \\ldots, 784$), la fonction de perte considérée (mesure de qualité) est le logarithme négatif de la vraisemblance défini par l'entropie croisée entre le score $\\text{score}(x)=(\\text{score}_0(x),\\dots,\\text{score}_9(x))$ et le vrai label $y=(y_0,\\dots,y_9)$:\n",
    "$$\n",
    "    \\ell\\big( \\text{score}(x), y \\big) = - \\sum_{k=0}^{9} y_{k} \\log\\big( \\text{softmax}(\\text{score}_k(x)) \\big).\n",
    "$$\n",
    "\n",
    "Pour ce premier modèle, nous optimisons par rapport aux paramètres $(\\boldsymbol{W}, \\boldsymbol{b})$ la perte totale $F(\\boldsymbol{W}, \\boldsymbol{b})$ sur l'ensemble d'entraînement $(x^i,y^i){1\\le i \\le n_{\\text{train}}}$ exprimée comme\n",
    "$$\n",
    "    F(\\boldsymbol{W}, \\boldsymbol{b}) = \\sum_{i=1}^{n_{\\text{train}}} \\ell \\big(\\text{score}(x^i), y^i \\big) = - \\sum_{i=1}^{n_{\\text{train}}} \\sum_{k=0}^{9} y^i_{k} \\log(p_k(x^i)).\n",
    "$$\n",
    "\n",
    "- Utiliser `nn.CrossEntropyLoss` pour la perte $\\ell$ sur le dataset `mnist_train`. \n",
    "\n",
    "- Créer un objet `optimizer` de type `optim.SGD` qui est une classe du module `torch.optim` de PyTorch qui implémente l'algorithme de descente de gradient stochastique (SGD) pour l'optimisation des paramètres d'un modèle. Le paramètre `lr` (learning rate) sera fixé à 0.005. \n",
    "\n",
    "- Faire une boucle de 10 itérations qui parcourt 10 fois tout le dataset (`x_train`,`y_train`). À chaque itération, vous réinitialisez les gradients avec `optimizer.zero_grad()`, calculez les prédictions du modèle (sur tout le dataset), calculez la perte, effectuez la rétropropagation du gradient avec `loss.backward()`, puis mettez à jour les paramètres avec `optimizer.step()`. Cela applique les mises à jour des paramètres basées sur le gradient calculé automatiquement (différentiation automatique aussi connue sous le nom de rétropropagation du gradient).\n",
    "\n",
    "- Une fois que le code fonctionne, passer à 200 epochs et tracer la loss, puis les accuracy (train et test) en fonction des epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402c73d-1102-43bf-91c4-61d7f64699df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Optimisation stochastique, mini-batch\n",
    "\n",
    "Ecrire une fonction pour faire l'apprentissage d'un modèle en implémentant un algorithme de descente de gradient stochastique. Adapter le code précédent pour écrire une fonction `training` qui prend pour arguments:\n",
    "- `model` un modèle qui renvoie les scores \n",
    "- `optimizer` (qu'on suppose bien initialisé avec les `model.parameters()`)\n",
    "- `epochs` le nombre d'epochs, par défaut à 100\n",
    "- `batch_size` par défaut à 512\n",
    "\n",
    "A chaque itération de `epoch` la procédure est la suivante: \n",
    "- on mélange le dataset (shuffle)\n",
    "- on parcourt le dataset par bloc de taille `batch_size`\n",
    "- sur chaque bloc on calcule le gradient et on met à jour les paramètres du modèle\n",
    "\n",
    "A la fin d'une itération `epoch` on recalcule `accuracy_train` et `accuracy_test` sur les données. L'**accuracy calculée est la proportion d'images bien prédites**.\n",
    "\n",
    "**Remarque:** en `PyTorch` on peut utiliser un `DataLoader` pour faire le travail de fournir les données en paquets de taille `batch_size` mais pour des raisons de pédagogie on refait le code à la main à partir de `x_train` et `y_train`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11084437-74e6-4c1e-8cc0-a74d5dac9836",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Comparaison SGD et Adam\n",
    "\n",
    "Utiliser la fonction précédente `training` pour comparer sur 200 epochs les 2 optimiseurs suivants:\n",
    "- SGD Stochastic Gradient Descent classique avec learning rate de 0.005\n",
    "- Adam (ADAptive Moment estimation) avec les paramètres par défaut\n",
    "\n",
    "On tracera les loss en fonction des epochs ainsi que l'accurary score sur les données d'entrainement et sur les données d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c90855a-e22a-49fd-9261-174025d51805",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Changement de backend (si possible)\n",
    "\n",
    "En informatique, le terme \"backend\" fait référence à la partie d'un logiciel ou d'un framework qui est responsable de l'exécution des opérations de bas niveau, telles que les calculs mathématiques, l'optimisation et l'interaction avec le matériel. En `PyTorch`, un backend est la composante responsable de l'exécution des opérations sur les tenseurs et de la gestion des calculs.\n",
    "`PyTorch` propose plusieurs backends pour exécuter les opérations de bas niveau. Les principaux backends de `PyTorch` sont les suivants :\n",
    "\n",
    "- CPU Backend: `\"cpu\"` les calculs sont effectués sur le processeur central (CPU) de l'ordinateur. Il offre une exécution efficace sur les CPUs modernes et est capable de tirer parti des optimisations spécifiques du matériel.\n",
    "\n",
    "- CUDA Backend: `\"cuda\"` CUDA est une plateforme de calcul parallèle développée par NVIDIA. `PyTorch` utilise le backend CUDA pour exécuter des calculs sur les GPU NVIDIA. L'utilisation de GPU permet d'accélérer considérablement les calculs en parallélisant les opérations sur des milliers de coeurs de traitement. Cela rend `PyTorch` particulièrement efficace pour l'entraînement et l'inférence de modèles de réseaux de neurones profonds.\n",
    "\n",
    "- Autres backends: `\"mps\"` pour le GPU intégré des puces M1/M2 chez Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf84a7-ccb4-481b-bb14-bdaa8f431761",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Feed-forward neural network (FFNN)\n",
    "\n",
    "On ajoute une couche cachée (hidden layer) entre les entrées et la couche de sortie linéaire de 10 neurones. Cette couche cachée est une couche linéaire avec une fonction non-linéaire appliquée point par point. La fonction non-linéaire classique que l'on utilise ici est la fonction **Rectified Linear Unit**: $\\text{ReLU}(x) = \\max(x, 0)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d9fdc6-172e-4493-9b73-76fb0611e5ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Définition du modèle\n",
    "- Définir un `model_ffnn` avec une seule couche cachée de 128 neurones. Cette couche a pour fonction d'activation un ReLU.\n",
    "- Combien de paramètres possède cette fonction ? C'est la dimension dans lequel on doit résoudre le problème d'optimisation! \n",
    "- Reprendre les questions précédentes avec ce modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0161b729-76e2-4791-92ef-6e8e5d04b862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e56173-9888-49ef-8de2-e4c26259dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1440b-9402-46be-b81e-225aa35265fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Visualisation des exemples mal classés\n",
    "\n",
    "- Combien d'exemples sont mal classés par ce modèle après apprentissage et optimisation via Adam ? \n",
    "- Visualiser quelques exemples mal classés par ce modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b4f1a-1344-4bf6-95c7-9518d1ded813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dde7cff3-f901-45d7-9621-8a0996193ab3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Convolutional Neural Network (CNN)\n",
    "\n",
    "Un réseau neuronal convolutif (Convolutional Neural Network ou CNN en anglais) est une architecture spécifique de réseau neuronal profond (deep learning) conçue principalement pour le traitement des données structurées en grille, telles que des images ou des séquences temporelles. Les CNN sont particulièrement performants dans les tâches de vision par ordinateur, comme la classification d'images, la détection d'objets et la segmentation sémantique.\n",
    "\n",
    "La principale caractéristique d'un CNN réside dans son utilisation de couches de convolution. Les couches de convolution effectuent des opérations de convolution sur les entrées, en utilisant des filtres ou des noyaux appris pour extraire des caractéristiques spécifiques des données: les bords, les textures ou les motifs présents dans les données.\n",
    "\n",
    "Une couche de convolution possède plusieurs hyperparamètres: \n",
    "\n",
    "En plus des couches de convolution, les CNN comprennent également d'autres types de couches:\n",
    "\n",
    "- Couches de pooling : Les opérations de pooling, comme le max pooling, aident à préserver les caractéristiques les plus importantes tout en réduisant la quantité de calculs nécessaires et le nombre de paramètres.\n",
    "\n",
    "- Couches de normalisation : Elles normalisent les activations des neurones pour améliorer la stabilité et accélérer l'apprentissage.\n",
    "\n",
    "- Couches entièrement connectées : Elles sont situées à la fin du réseau et réalisent une classification ou une régression sur les caractéristiques extraites par les couches précédentes. Ces couches sont similaires aux couches d'un réseau neuronal traditionnel et utilisent généralement des fonctions d'activation non linéaires telles que ReLU (Rectified Linear Unit).\n",
    "\n",
    "![](img/cnn_view.png)\n",
    "\n",
    "Voici par exemple des architectures \"historiques\", obtenues sur la page [wikipedia de LeNet](https://en.wikipedia.org/wiki/LeNet).\n",
    "![](img/cnn_examples.svg)\n",
    "\n",
    "Ici on propose de tester une architecture basée sur: \n",
    "- `nn.Conv2d`: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "- `nn.MaxPool2d`: https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "- `nn.Linear`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40fcdd-dce9-43b2-bcdf-aefadf02cf76",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Définition du modèle \n",
    "\n",
    "Définir le réseau CNN suivant:\n",
    "- Convolution avec 8 kernels de taille 5 (c'est à dire un carré 5x5), stride et padding à 0\n",
    "- ReLU\n",
    "- Max-Pooling 2x2\n",
    "- Convolution avec 16 kernels de taille 5, stride et padding à 0\n",
    "- ReLU\n",
    "- Max-Pooling 2x2\n",
    "- Une simple couche linéaire de sortie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056e187-e240-4134-93cb-0da80d9bb720",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Changement de dataset: FashionMNIST\n",
    "\n",
    "Refaire la comparaison FNN (fully-connected) vs CNN sur le jeu de données FashionMNIST.\n",
    "```\n",
    "datasets.FashionMNIST\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3308334-f8d6-49f7-8b4b-3a6cf25a345a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "mnist_train = datasets.FashionMNIST('data', train=True, download=True)\n",
    "mnist_test = datasets.FashionMNIST('data', train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d1e5e2-954b-420f-bb07-7fd6075b3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(ncols=6, nrows=3)\n",
    "for ax in axs.flatten():\n",
    "    idx = torch.randint(len(mnist_train), size=(1,)).item()\n",
    "    image, label = mnist_train[idx]\n",
    "    ax.imshow(image, cmap=\"gray\")\n",
    "    ax.set_axis_off()\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f856bc52-2dbf-4d9c-ace9-aadf952064a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Annexe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11d0e56-e2c4-4c1d-aab6-75023ac39e5e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Utilisation du `DataLoader`\n",
    "\n",
    "Le `DataLoader` de `PyTorch` est une classe utilitaire qui facilite le chargement et la gestion des données d'entraînement et de test dans les modèles d'apprentissage automatique. Il offre plusieurs avantages et fonctionnalités qui simplifient le processus de préparation des données et d'itération sur les batchs (mini-lots) lors de l'entraînement des modèles. \n",
    "\n",
    "On reprend le jeu de données MNIST.\n",
    "- Créer un `DataLoader` en spécifiant le jeu de données `mnist_train` comme argument, avec un `batch_size` de 64 pour regrouper les données en mini-lots de taille 64, et avec l'option `shuffle=True` pour mélanger aléatoirement les données à chaque epoch (après un passage complet sur l'ensemble du dataset).\n",
    "- Faire une boucle sur cet objet `DataLoader` pour le manipuler.\n",
    "- Que représente une itération complète sur cet objet, et combien d'images est dans le dernier lot ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cca102-cbe3-4f9b-8b96-d48ccd0ee601",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e2da8-00b6-4088-ba07-9cb6497145e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
