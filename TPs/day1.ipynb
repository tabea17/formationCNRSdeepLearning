{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0fb83e-2bbc-4a2a-9462-0b35fdae56a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# TP: Régression logistique et `scikit-learn` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e053ea4-f29f-4e6c-9972-8754cfdf8c2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Dans ce premier TP, nous mettons en oeuvre un modèle de régression logistique pour la prédiction de labels binaires $y \\in \\{0,1\\}$. On rappelle que la régression logistique repose sur une modélisation probabiliste, et donc plutôt que de prédire simplement 0 ou 1, on construit un estimateur $\\hat P(y=1|x)$ de la probabilité $\\mathbf{P}(y=1|x)$ et on définit un classifieur comme\n",
    "$$\\hat y (x_{new}) = \\begin{cases} 1 & \\text{si } \\hat P(y=1|x_{new}) \\geq t, \\\\ 0 & \\text{sinon}\\end{cases}$$\n",
    "pour un seuil $t\\in(0,1)$ donné. Par défaut, si les labels jouent un rôle symmetrique, $t=1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1dfc4-bd4f-48c0-8865-733e8a1d4988",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Le jeu de données\n",
    "\n",
    "Les données utilisées dans ce TP proviennent d'un jeu de données classique qui s'appelle *Census Income dataset* (également connu sous le nom de *Adult Income* et disponible sur le site [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)).\n",
    "\n",
    "Ce dataset a été extrait par Barry Becker du recensement américain de 1994 et est souvent utilisé pour des tâches de classification binaire, où l'objectif est de **prédire si un individu gagne plus ou moins de $50\\,000$ dollars par an**, en se basant sur les caractéristiques démographiques et professionnelles disponibles. Les **caractéristiques sont qualitatives et quantitatives** et incluent l'âge, le niveau d'éducation, le statut matrimonial, la profession, le pays d'origine, le sexe, le capital investi, le nombre d'heures de travail par semaine, etc.\n",
    "\n",
    "Le fichier `data/adult_clean.csv` a été obtenu en nettoyant les fichiers publics (voir le notebook `day1_preparation_data.ipynb`): on a retiré les données manquantes et les doublons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3608f5b-8a81-4d35-a6c0-23ec7be9f468",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "On utilise le module `pandas` qui est une bibliothèque open-source populaire en Python, largement utilisée pour la manipulation et l'analyse des données. Ce module fournit des structures de données flexibles et performantes: \n",
    "- les **DataFrames** qui sont similaires aux tables de base de données ou aux feuilles de calcul Excel\n",
    "- les **Series** qui sont des tableaux unidimensionnels avec étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef5b9e-2c41-43b9-a166-3dcf7a1a032e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d27f1f-aa1b-4a93-a307-3212e149f3cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 1 : Chargement \n",
    "\n",
    "- Importer les données du fichier `data/adult_clean.csv` sous forme d'un `DataFrame` pandas que l'on nomme `dataset`. \n",
    "- Visualiser les premières lignes de `dataset`, noter le nombre de caractéristiques (et leurs noms) et le nom de la variable à prédire.\n",
    "- Quel est le nombre total de données (de lignes) du `dataset`, et la proportion de données `<=50K` et `>50K`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0002eaa-432a-4a93-89f9-bacec7428b0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c12c7e74-7af2-4d8c-b63b-8482610c985d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 2 : Exploration, visualisation \n",
    "\n",
    "Familiarisez-vous avec les données.\n",
    "\n",
    "- Calculer quelques statistiques descriptives (à l'aide de `shape`, `dtypes`, `describe`, `value_counts`).\n",
    "- Visualiser quelques caractéristiques en fonction des 2 groupes `<=50K` et `>50K`.\n",
    "\n",
    "Pour tracer des graphiques on pourra utiliser les librairies `matplotlib` et `seaborn` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9408ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2669ab2b",
   "metadata": {},
   "source": [
    "On pourra utiliser les fonctions suivantes :\n",
    "- `sns.scatterplot` pour des nuages des points\n",
    "- `sns.histplot` pour des histogrammes et des diagrammes en bâtons\n",
    "\n",
    "**Exemple** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2667ebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.histplot(data=dataset, x=\"sex\", hue=\"class\", \n",
    "                  multiple=\"stack\", alpha=0.5, discrete=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2447e796-a425-42b8-98d0-6488940a64b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1fe5ce5-83b0-4684-8ec4-adfe5647cee8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 3 : Pré-traitement des données\n",
    "\n",
    "On ne travaille que très rarement sur des donnnées brutes. \n",
    "\n",
    "Pour des  **variables quantitatives**, il est important de les  **standardiser** (centrer et réduire) afin de les ramener à  des échelles comparables. Cela diminue également les problèmes numériques. \n",
    "\n",
    "Pour une **variable qualitative** ou **catégorielle**, il n'y a en général pas  d'ordre naturel de ses modalités. Donc il ne faut pas bêtement remplacer des catégories *A*, *B*, *C*,... par 1, 2, 3,... Le problème est que dans un modèle de régression, on va calculer par exemple des moyennes sur ces valeurs, mais en général *B* n'est pas la moyenne de *A* et *C* (alors que c'est le cas pour la valeur 2 par rapport à 1 et 3). Il nous faut alors un encodage qui est invariant à l'ordre des modalités. On utilise  le **one-hot encoding** qui consiste à transformer une variable qualitative en plusieurs variables binaires dites **dummies**. Une variable avec K modalités est transformée en K-1 variables binaires. Chacune de ces variables binaires indique pour une catégorie spécifiée si la variable observée est égale à cette catégorie ou pas. Il suffit de K-1 variables binaires pour encoder K catégories, car l'information sur la K-ième catégorie peut être déduite des K-1 autres variables binaires. Donc, si on utilisait K variables binaires, on introduirait des corrélations entre les colonnes, ce qu'il faut éviter dans un cadre de régression.\n",
    "\n",
    "- Séparer les données en 3 dataframes qu'on nomme :\n",
    "\n",
    "    - `quantitative`: dataframe avec toutes les covariables quantitatives\n",
    "    - `qualitative`: dataframe avec toutes les covariables qualitatives\n",
    "    - `category` : pour les labels observés `class`\n",
    "\n",
    "En vue de la régression logistique on effectue le pré-traitement suivant: définir les variables \n",
    "- `labels` en remplaçant les classes `<=50K` et `>50K` par les valeurs numériques 0 et 1\n",
    "- `quantitative_norm` obtenu en renormalisant le dataset `quantitative`\n",
    "- `qualitative_enc` obtenu par l'appel de `get_dummies` du module `pandas` sur le dataset `qualitative` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2524e14-9b1b-4775-b355-014bde9b24d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a96f23bb-8349-4b32-8346-82b5c6e3cef9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Echantillons : apprentissage et test \n",
    "\n",
    "On rappelle que la modélisation se fait en trois temps :\n",
    "- on sépare les données : TRAIN / TEST\n",
    "- on apprend le modèle sur TRAIN\n",
    "- on évalue la performance du modèle appris sur TEST\n",
    "\n",
    "On coupe les données de façon aléatoire en deux groupes. \n",
    "Le plus souvent, on les sépare en 80% pour l'apprentissage et 20% pour le test. D'autres pourcentages courants sont 67-33 ou 50-50.\n",
    "On   utilise la  fonction `train_test_split` du package `sklearn.model_selection` pour séparer aléatoirement les données. \n",
    "\n",
    "Même si le split est aléatoire, on souhaite que les deux échantillons soient tous les deux représentatifs du problème. En particulier, on voudrait qu'ils contiennent le même pourcentage de labels 0 et 1, ce qui est notamment important quand  les labels ne sont pas équilibrés (pas 50-50).\n",
    "\n",
    "Ici, la proportion de 1 (individus qui sont dans la catégorie `>50K`) est d'environ 0.25 (le vérifier!).\n",
    "\n",
    "Si le taux n'est pas très élevé, une coupe totalement aléatoire des données risque de produire des sous-échantillons où l'un des deux ne contient que peu de labels qui valent 1. Cela peut être problématique pour l'apprentissage du modèle comme pour l'évaluation de la méthode.\n",
    "\n",
    "En pratique, dans des problèmes de classification, on force alors la même répartition des labels dans les deux échantillons TRAIN et TEST, via l'option `stratify`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ce0cd-5728-475e-83f0-38b8a47d0de7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 4 : Création des échantillons \n",
    "\n",
    "Utiliser la fonction `train_test_split` du module `scikit-learn` pour créer à partir de nos données `quantitative_norm`, `qualitative_enc` et `labels` les 4 variables suivantes: \n",
    "- `x_train` et `y_train` qui contiennent 80% du jeu de données pour l'apprentissage du modèle de régression logistique \n",
    "- `x_test` et `y_test` qui contiennent les 20% restants pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc8b3c-f2d0-40c5-9444-83ca2e43e9b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7364ecb2-df56-467f-aa85-a59da6fbea65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Régression logistique\n",
    "\n",
    "Le but de ce TP est de fitter un modèle de régression logistique à ces données afin de prédire le label (0 pour `<=50K` ou 1 pour `>50K`) pour des  nouveaux individus à partir de leurs caractéristiques. Nous commençons par le modèle simple qui prend en compte toutes les variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2a58c-6df9-4f9f-8ae4-7c0cf69bfa9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 5 : Création du modèle et estimation\n",
    "\n",
    "- Utiliser la fonction `LogisticRegression` de `sklearn.linear_model` pour effectuer une régression logistique simple (i.e. sans pénalisation). Fitter le modèle sur les données `train`. \n",
    "- Visualiser les coefficients estimés (intercept et variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c26d1b7-bffe-4e15-ba88-2d6db1464f42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd8c50f7-a3bf-4e67-8c34-726433b30f6b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 6 : Prédiction\n",
    "\n",
    "La méthode `predict_proba` renvoie 2 colonnes: la première est la probabilité, conditionnellement aux caractéristiques de chaque individu, d'être `<50K` (label 0, négatif) et la seconde la probabilité, conditionnellement aux caractéristiques de chaque individu, d'être `>=50K` (label 1, positif). A partir de ces probas on peut \"décider\" en fixant un seuil $t \\in [0,1]$ d'être ou non dans une des 2 catégories en utilisant le classifieur construit à partir de ces probabilités : \n",
    "$$\\hat y (x_{new}) = \\begin{cases} 1 & \\text{si } \\hat P(y=1|x_{new}) \\geq t, \\\\ 0 & \\text{sinon}\\end{cases}$$\n",
    "\n",
    "- Sur le jeu `test`, calculer $\\hat P(y=1|x)$, les probabilités que `>50K` pour chaque individu $x$ (`predict_proba()`) et les prédictions $\\hat y(x)$ (`predict()`) que l'on obtient par seuillage des probabilités au seuil de $t = 1/2$.\n",
    "- Comparer le nombre moyen de positifs prédits par rapport à celui connu dans `y_test`. \n",
    "- Visualiser la distribution des probabilités prédites.\n",
    "- Faire varier le seuil $t \\in [0,1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b48460-29fa-4088-b2d3-42a957d9570c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6d725b-a51c-414d-b217-2afbc962f5ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Mesures de qualité du classifieur \n",
    "\n",
    "Le module `metrics` de `scikit-learn` contient plusieurs fonctions qui permettent d'évaluer la qualité des prédictions d'un modèle. Ces métriques sont détaillées dans les sections sur les métriques de classification, les métriques de classement multi-label, les métriques de régression et les métriques de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2835f5-2761-4b73-bbbf-0bf4a36a9eaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(dir(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f690cb-f057-4877-b9bf-c901b9c1a317",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Erreurs de classification \n",
    "Pour chaque individu $i$ de caractéristiques $x_i$ dans l'échantillon, nous avons : \n",
    "- un vrai label/valeur $y_i$\n",
    "- un label/valeur prédit $\\hat y_i$\n",
    "\n",
    "Deux type d'erreurs peuvent se produire :\n",
    "1. Prédire $\\hat y_i =1$ alors que $y_i=0$ (faux positif/false positive, FP)\n",
    "2. Prédire $\\hat y_i =0$ alors que $y_i=1$ (faux négatif, false negative, FN)\n",
    "\n",
    "Dans un contexte pratique, ces deux erreurs ne sont pas symétriques, parce qu'une erreur peut porter à des  effets plus négatifs que l'autre. En général, les labels sont choisis de sorte à être moins tolérants aux faux positifs qu'aux faux négatifs, pensons par exemple aux tests de grossesse, ou aux procès en justice. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02d6e1-9b21-4bc0-a265-7859b4d48272",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### EX 7 : Accuracy score  \n",
    "\n",
    "Le taux de précision (_accuracy score_) est la mesure de performance qui représente la proportion d'observations correctement classées par le modèle parmi l'ensemble total des observations. Ce taux est un pourcentage, où une valeur de 100% signifie que toutes les prédictions du modèle sont correctes.\n",
    "\n",
    "- Implémenter à la main (en `numpy`) le calcul de ce score pour les données de test.\n",
    "- Obtenir ce score via l'appel de `accuracy_score` du module `sklearn.metrics` et via la méthode `score` de l'objet `model`.\n",
    "- Comparer ce score à un estimateur trivial qui consiste à toujours renvoyer la classe la plus fréquente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723c34c-8efa-4cef-9c39-44a38ba9d026",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64330509-5962-4b0a-8964-b09a03a9cb44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Matrice de confusion, recall, precision\n",
    "\n",
    "La matrice de confusion permet de visualiser les performances du modèle en termes de prédictions correctes (vrais positifs et vrais négatifs) et d'erreurs de prédiction (faux positifs et faux négatifs). La représentation classique se fait sous forme d'une matrice où les nombres sur la diagonale sont liés aux prédictions correctes, tandis que les nombres hors de la diagonale sont liés aux prédictions incorrectes. \n",
    "\n",
    "![](img/confusion-matrix-python.jpg)\n",
    "\n",
    "Plus précisément: \n",
    "\n",
    "- le coin inférieur droit représente les **vrais positifs** (TP): les personnes de la classe `>50K` prédites comme telles par le modèle \n",
    "- le coin supérieur gauche représente les **vrais négatifs** (TN): les personnes de la classe `<=50K` prédites comme telles par le modèle \n",
    "- le coin supérieur droit représente les **faux positifs** (FP): les personnes de la classe `<=50K` prédites comme étant `>50K`\n",
    "- le coin inférieur gauche représente les **faux négatifs** (FN): les personnes de la classe `>50K` prédites comme étant `<=50K`\n",
    "\n",
    "**Attention**:\n",
    "Dans la page wikipedia la matrice est \"inversée\": les vrais positifs sont en haut à gauche... \n",
    "\n",
    "A partir de ces données on peut construire plusieurs indicateurs de performance, dont le recall et la précision.\n",
    "\n",
    "#### Recall ou sensitivité\n",
    "\n",
    "$$\\frac{\\text{TP}}{\\text{\\# (real P)}} = \\frac{\\text{TP}}{\\text{TP+FN}}$$\n",
    "Mesure la capacité du classifieur à identifier tous les exemples positifs réels, c'est-à-dire sa sensibilité ou son taux de détection. Dans notre cadre, le recall est le taux d'individus identifiés comme `>50K` par le classifeur parmi tous les individus avec `>50K` dans `y_test`.\n",
    "\n",
    "#### Precision\n",
    "$$\\frac{\\text{TP}}{\\text{\\# (predicted P)}} = \\frac{\\text{TP}}{\\text{TP+FP}}$$\n",
    "Mesure la capacité du classifieur à ne prédire positif que lorsque la prédiction est vraiment correcte.  P. ex.  le classifieur qui prédit systématiquement `>50K` pour tous les individus a un  recall  de 100%, mais sa precision est de 0%. Donc, on veut que les deux scores, le recall et la precision, d'un classifieur soient élevés.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c2615",
   "metadata": {},
   "source": [
    "### EX 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844f839-eb19-4152-b3d0-50fdb6b48a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "- Tracer la matrice de confusion en utilisant `ConfusionMatrixDisplay` et donner les scores `recall` et `precision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5386e743-fdff-49c2-adb2-cf82dd25b6aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4bfa766-8b94-41bb-8a92-ec3529bda985",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Courbe ROC, AUC\n",
    "\n",
    "Les valeurs prédites `y_predicted` ci-dessus ont été obtenues en seuillant les probabilités de `>50K` calculées par le modèle de régression logistique avec le seuil par defaut $t=1/2$.\n",
    "\n",
    "$$\\hat y_t (x_{new}) = \\begin{cases} 1 & \\mathbf{\\hat P}(y=1 | x_{new}) \\geq t, \\\\ 0 & \\text{sinon.}\\end{cases}$$\n",
    "\n",
    "Or, en variant le seuil $t\\in(0,1)$ il est possible que le classifieur associé ait des meilleurs scores.\n",
    "\n",
    "La courbe ROC (Receiver Operating Characteristic) est une courbe paramétrique, paramétrée par $t \\in [0,1]$. Elle représente les points $(\\text{FPR}_t, \\text{TPR}_t)$ pour tout seuil $t\\in[0,1]$, où \n",
    "\n",
    "- $\\text{TPR}_t$ (True positive rate) est le recall en fonction du seuil $t$ :\n",
    "$$\\frac{\\text{TP}_t}{\\text{TP}_t+\\text{FN}_t}$$\n",
    "\n",
    "- $\\text{FPR}_t$ (False positive rate) est définit par la ratio suivant en fonction du seuil $t$ :\n",
    "$$\\frac{\\text{FP}_t}{\\text{FP}_t+\\text{TN}_t}$$\n",
    "\n",
    "Avec le seuil $t=1$, le classifieur prédit `<=50K` systématiquement, donc il n'y a pas de positifs et on a $\\text{TP}_0 = 0$ et $\\text{FP}_0 = 0$. En revanche, avec un seuil de $t=0$, le classifieur prédit toujours `>50K` et donc on a $\\text{TP}_1 = 1$ et $\\text{FP}_1 = 1$. Toutes les courbes ROC commencent donc au point $(1, 1)$ et se terminent au point $(0,0)$. La fonction $y=x$ represente la courbe ROC de la famille di classifieurs $\\hat y_t (x_{new}) = 1$ avec probabilité $t\\in(0,1)$.\n",
    "\n",
    "Si la courbe est élevée, le modèle a une très bonne performance quelque soit le seuil. Cela est mesuré par l'aire sous la courbe (AUC -- Area Under the ROC curve). L'AUC permet de comparer deux modèles, ou un modèle sur des jeux de données différentes, comme les données train et test.\n",
    "\n",
    "### EX 9\n",
    "- Utiliser la fonction `RocCurveDisplay` pour tracer la courbe ROC de notre modèle pour les données train et les données test. Sur quelles données observe-t-on une meilleure performance ?\n",
    "- Utiliser la fonction `roc_auc_score` pour calculer l'AUC sur les données de test et vérifier qu'elle correspond à l'AUC indiquée sur le tracé de courbe au point précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5e6cd-6598-4ff9-a448-94996289098b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "badcd5fe-6213-4b93-a6e2-c64a2b1eea5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Courbe precision/recall, AP\n",
    "\n",
    "Un autre outil de comparaison est la courbe precision/recall qui représente les points des coordonnées (recall, precision) pour tout seuil $t\\in[0,1]$. L'interprétation est la même : plus la courbe est élevée (ou plus l'aire sous la courbe est grande), mieux c'est.\n",
    "\n",
    "Noter que contrairement à la courbe ROC, la courbe precision/recall n'est pas toujours monotone.\n",
    "\n",
    "### EX 10\n",
    "- Tracer la courbe precision/recall de notre modèle sur les données test en utilisant la fonction `PrecisionRecallDisplay`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7176ab48-c998-48e3-98f5-202afd9c27fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4455942b-9641-44fe-93a1-1a6feb3023bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Régression logistique avec pénalisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ec628-f40a-4496-ada3-c6d56f9ab959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Régression ridge\n",
    "\n",
    "Nous allons utiliser une pénalité en norme 2 (dite $\\ell_2$ ou Ridge) sur les coefficients de la régression. Cette pénalité Ridge a tendance à \"rétrecir\" les valeurs des coefficients (on parle de shrinkage) et régularise la solution.\n",
    "\n",
    "### EX 11\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_2$ sur les données d'apprentissage. On utilisera toujours la fonction `LogisticRegression`avec  l'option `penalty='l2'`. On utilisera une pénalité assez forte, p. ex. `C=0.01`. \n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité (visualisez l'effet de shrinkage). \n",
    "- Évaluer les performances du classifieur sur les données de test.\n",
    "- Pour les deux classifieurs (avec et sans pénalisation $\\ell_2$), comparer les courbes de score sur les données `test` ainsi que les performances en termes de courbes (ROC ou precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab3def-c5c8-48df-9bee-cb01916a93a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f01430d-5a6d-45af-b729-3c5dc4d4787c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Régression lasso\n",
    "\n",
    "Dans cette partie, on va mettre en oeuvre une pénalité $\\ell_1$ dans la régression logistique. Cela peut permettre de faire de la sélection de variables : les variables peu informatives pour la prédiction ne seront plus du tout utilisées (coefficient $w_i$ estimé à 0). On peut ainsi construire un estimateur plus parcimonieux. \n",
    "\n",
    "### EX 12\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_1$ sur les données d'apprentissage (avec l'option le `solver='liblinear'`).\n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité et avec pénalité $\\ell_2$ (visualiser l'effet d'annulation des coefficients). Tester différentes valeurs de $C$.\n",
    "- Pour l'ensemble des classifieurs (avec et sans pénalisation $\\ell_1$ et $\\ell_2$), comparer les performances en termes de courbes (ROC et precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22ec9f-a99e-4ae0-bd50-aa9121bb02ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ee27d3-508f-4868-ae28-01de652b8b06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Chemins de régularisation pour le Lasso\n",
    "\n",
    "Le but est de visualiser l'effet de la **constante de pénalisation** sur l'évolution des coefficients $w_i$, à travers les **chemins de régularisation**. Lorsque la pénalité est très forte (i.e $C$ très petite), aucune  variable n'est sélectionnée (tous les coefficients $w_i$ sont estimés à 0). Dans ce cas, la fonction de régression logistique est constante (on a seulement l'intercept) et le lien entre $y$ et $X$ est très mal appris. Puis au fur et à mesure que $C$ augmente, on inclue de plus en plus de variables dans notre modèle de régression logistique. Lorsque $C$ est très grande, on ne pénalise plus et on retrouve les résultats de la régression logistique simple. \n",
    "\n",
    "### EX 13\n",
    "\n",
    "- Fixer une grille de valeurs de $C$, estimer le modèle lasso pour chaque constante $C$ et afficher les coefficients en fonction de $C$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a6e37-2f6b-4c1e-a199-0ad0457935f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c3755e-c135-4be8-b42b-436a9f6d9400",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Validation croisée\n",
    "\n",
    "La validation croisée consiste à entrainer le modèle sur différents sous-échantillons des données et d'évaluer à chaque fois sa capacité prédictive sur le reste des données. \n",
    "La validation croisée permet de choisir la meilleure constante $C$ de la pénalité lasso ou Ridge. Plus généralement, dans de nombreux autres modèles, la validation croisée est utilisée pour ajuster les hyperparamètres du modèle de façon optimale en évitant le surapprentissage et augmentant sa capacité de généralisation sur des nouvelles données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10acd2-ba45-4c3e-9ae8-b0136869e88e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 14 :  Stabilité des prédictions\n",
    "\n",
    "- Utiliser la fonction `cross_validate` du module `sklearn.model_selection` pour obtenir un intervale de confiance de l'accuracy score. On fera cette étude sur le modèle de régression logistique simple `model` et celui avec pénalité $\\ell_1$ `model_lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92645455-484a-4f3e-a576-aba15b924ebc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8953754d-4d18-4561-b3be-111edb7bf945",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Choix de la constante de pénalisation\n",
    "\n",
    "On peut aussi utiliser la validation croisée pour choisir un hyperparamètre. Dans cette partie, nous allons mettre en oeuvre le choix de la constante de pénalisation par **validation croisée**. \n",
    "Nous le ferons dans le cadre d'une pénalité $\\ell_1$, mais on pourrait faire exactement la même chose avec un autre type de pénalité. \n",
    "\n",
    "Par défaut, la fonction `LogisticRegression` ne sait pas choisir la constante de pénalisation automatiquement et par défaut cette constante est fixée à 1. Ce choix n'a pas de justification et n'a donc aucune raison d'être utilisé. \n",
    "\n",
    "### EX 15\n",
    "\n",
    "- Utiliser la fonction `LogisticRegressionCV` pour choisir la constante $C$ qui maximise le score AUC (aire sous la courbe ROC) dans une grille de valeur de $C$ fixée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416e68c1-341b-4c92-a68f-3aa37d1dc55f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f131029-8890-4077-be5f-dd2bb57c9be4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## `scikit-learn` et automatisation: `pipeline`\n",
    "\n",
    "En `scikit-learn`, un _pipeline_ est une séquence ordonnée d'étapes de prétraitement des données et de modélisation regroupées en une seule entité. Il permet de définir et d'automatiser un flux de travail cohérent pour le traitement des données et l'entraînement d'un modèle.\n",
    "\n",
    "Il se compose en général de plusieurs étapes:\n",
    "- transformations des données :  normalisation des variables, l'imputation des valeurs manquantes, la réduction de dimension, etc. Elles permettent de préparer les données avant de les fournir au modèle.\n",
    "- le modèle d'apprentissage automatique.\n",
    "- la validation croisée : pour évaluer les performances du modèle.\n",
    "\n",
    "L'avantage d'utiliser un pipeline est qu'il permet de regrouper toutes ces étapes en une seule entité. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d63ca5d-eb11-4a62-8fbc-d9306ad0ceb8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 16 : Création d'un pipeline pour notre régression logistique \n",
    "\n",
    "- Définir à partir des classes `OneHotEncoder`, `StandardScaler` de `sklearn.preprocessing` et `ColumnTransformer` de `sklearn.compose` un préprocesseur qui réalise les mêmes transformations que celles faites dans la section 1.1.5 (Pré-traitement des données). Attention, la transformation `OneHotEncoder` doit s'appliquer uniquement aux données qualitatives et la transformation `StandardScaler` doit s'appliquer uniquement aux données quantitatives. \n",
    "- Créer un pipeline avec la fonction `make_pipeline` de `sklearn.pipeline` pour définir un modèle de régression logistique avec prétraitement des données automatique (fait par le préprocesseur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a9d66-0542-4e06-add5-853ee25a941d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f1150e-82f9-4c17-9d48-1c3c0713fcec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 17 : Utilisation du pipeline \n",
    "\n",
    "- Faire une estimation du modèle. Attention, quelles données doivent être utilisées ?\n",
    "- Faire des prédictions et calculer les différents scores sur le jeu de test.\n",
    "- Faire une étape de validation croisée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec25cc-d083-48c5-baa9-5f2f83d2f17e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87974da9-76d8-445b-9aab-9191b02ead51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 18 : Avec une autre méthode de classification : Gradient Boosting\n",
    "\n",
    "L'avantage d'utiliser un module riche comme `scikit-learn` est qu'on peut facilement changer une brique pour tester d'autres modèles. Ici on propose de remplacer la brique `LogisticRegression` par une méthode basée sur les arbres de décisions. Les arbres de décisions et les méthodes de \"boosting\" seront expliquées dans le second cours mais voyons les résultats que l'on peut obtenir. \n",
    "\n",
    "- Utiliser le préprocesseur `OneHotEncoder` avec l'option `sparse_output` à `False` et la classe `HistGradientBoostingClassifier` du module `sklearn.ensemble` pour composer un pipeline que l'on nomme `model_gradboost`.\n",
    "\n",
    "- Comparer les courbes ROC et Precision/Recall obtenues avec cet estimateur et la régression logistique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77e7ce-5465-4142-92d0-65e4a6b372c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
