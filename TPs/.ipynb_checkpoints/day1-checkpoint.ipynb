{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0fb83e-2bbc-4a2a-9462-0b35fdae56a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# TP: Régression logistique et `scikit-learn` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e053ea4-f29f-4e6c-9972-8754cfdf8c2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Dans ce premier TP, nous mettons en oeuvre un modèle de régression logistique pour la prédiction de labels binaires $y \\in \\{0,1\\}$. On rappelle que la régression logistique repose sur une modélisation probabiliste, et donc plutôt que de prédire simplement 0 ou 1, on calcule pour chaque observation $x$ une probabilité dans $[0,1]$ qui correspond à un estimateur $\\mathbf{P}(y=1|x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1dfc4-bd4f-48c0-8865-733e8a1d4988",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Les données utilisées dans ce TP proviennent d'un jeu de données classique qui s'appelle \"Census Income\" dataset (également connu sous le nom de \"Adult Income\" et disponible sur le site [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)).\n",
    "\n",
    "Ce dataset a été extrait par Barry Becker du recensement américain de 1994 et est souvent utilisé pour des tâches de classification binaire, où l'objectif est de **prédire si un individu gagne plus ou moins de $50\\,000$ dollars par an**, en se basant sur les caractéristiques démographiques et professionnelles disponibles. Les **caractéristiques sont qualitatives et quantitatives** et incluent l'âge, le niveau d'éducation, le statut matrimonial, la profession, le pays d'origine, le sexe, le capital investi, le nombre d'heures de travail par semaine, etc.\n",
    "\n",
    "Le fichier `data/adult_clean.csv` a été obtenu en nettoyant les fichiers publics (voir le notebook `day1_preparation_data.ipynb`): on a retiré les données manquantes et les doublons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22bf96-9c70-4a28-a0d5-aecda59552ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Chargement des données\n",
    "\n",
    "On utilise le module `pandas` qui est une bibliothèque open-source populaire en Python, largement utilisée pour la manipulation et l'analyse des données. Ce module fournit des structures de données flexibles et performantes: \n",
    "- les **DataFrames** qui sont similaires aux tables de base de données ou aux feuilles de calcul Excel\n",
    "- les **Series** qui sont des tableaux unidimensionnels avec étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef5b9e-2c41-43b9-a166-3dcf7a1a032e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d27f1f-aa1b-4a93-a307-3212e149f3cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question\n",
    "\n",
    "- Importer les données du fichier `data/adult_clean.csv` sous forme d'un `DataFrame` pandas que l'on nomme `dataset`. \n",
    "- Visualiser les premières lignes de `dataset`, noter le nombre de caractéristiques (et leurs noms) et le nom de la variable à prédire.\n",
    "- Quel est le nombre total de données (de lignes) du `dataset`, et la proportion de données `<=50K` et `>50K`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3151de-97c2-4170-9d93-71b9e06b9b4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a5bb44-a917-4ca9-9880-468df7a9fb21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "**Remarque** : Pour lister toutes les méthodes/attributs utilisables pour un objet on peut utiliser la fonction `dir` par exemple \n",
    "```python\n",
    "print(dir(dataset))\n",
    "```\n",
    "Pour ne lister que les méthodes/attributs *publiques* on peut filtrer le résultat par exemple \n",
    "```python\n",
    "print([m for m in dir(dataset) if m.startswith('_') == False])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e04c5-c960-4f44-b69e-e9ab3e84cee2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Exploration des données "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12c7e74-7af2-4d8c-b63b-8482610c985d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question \n",
    "\n",
    "- Familiarisez-vous avec les données (à l'aide de `shape`, `dtypes`, `describe`, `value_counts`).\n",
    "- Visualiser quelques caractéristiques en séparant les 2 groupes `<=50K` et `>50K`.\n",
    "- Séparer les données en 3 dataframes qu'on nomme :\n",
    "\n",
    "    - `quantitative`: dataframe avec toutes les covariables quantitatives\n",
    "    - `qualitative`: dataframe avec toutes les covariables qualitatives\n",
    "    - `category` : pour les labels observés `class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a35c5-d829-4d5b-bdf8-7975fce0217b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe5ce5-83b0-4684-8ec4-adfe5647cee8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Pré-traitement des données\n",
    "\n",
    "On ne travaille que très rarement sur des donnnées brutes. \n",
    "\n",
    "Pour des  **variables quantitatives**, il est important de les  **standardiser** (centrer et réduire) afin de les ramener à  des échelles comparables. Cela diminue également les problèmes numériques. \n",
    "\n",
    "Pour une **variable qualitative** ou **catégorielle**, il n'y a en général pas  d'ordre naturel de ses modalités. Donc il ne faut pas bêtement remplacer des catégories *A*, *B*, *C*,... par 1, 2, 3,... Le problème est que dans un modèle de régression, on va calculer par exemple des moyennes sur ces valeurs, mais en général *B* n'est pas la moyenne de *A* et *C* (alors que c'est le cas pour la valeur 2 par rapport à 1 et 3). Il nous faut alors un encodage qui est invariant à l'ordre des modalités. On utilise  le **one-hot encoding** qui consiste à transformer une variable qualitative en plusieurs variables binaires dites **dummies**. Une variable avec K modalités est transformée en K-1 variables binaires. Chacune de ces variables binaires indique pour une catégorie spécifiée si la variable observée est égale à cette catégorie ou pas. Il suffit de K-1 variables binaires pour encoder K catégories, car l'information sur la K-ième catégorie peut être déduite des K-1 autres variables binaires. Donc, si on utilisait K variables binaires, on introduirait des corrélations entre les colonnes, ce qu'il faut éviter dans un cadre de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558315b7-64f5-4e07-83b5-6ed57ed335f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question \n",
    "\n",
    "En vue de la régression logistique on effectue le pré-traitement suivant: définir les variables \n",
    "- `labels` en remplaçant les classes `<=50K` et `>50K` par les valeurs numériques 0 et 1\n",
    "- `quantitative_norm` obtenu en renormalisant le dataset `quantitative`\n",
    "- `qualitative_enc` obtenu par l'appel de `get_dummies` du module `pandas` sur le dataset `qualitative` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e31f1-3342-4ebd-a992-e5cf0627e609",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f23bb-8349-4b32-8346-82b5c6e3cef9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Echantillons: apprentissage et test \n",
    "\n",
    "On rappelle que la modélisation se fait en trois temps :\n",
    "- on sépare les données : TRAIN / TEST\n",
    "- on apprend le modèle sur TRAIN\n",
    "- on évalue la performance du modèle appris sur TEST\n",
    "\n",
    "On coupe les données de façon aléatoire en deux groupes. \n",
    "Le plus souvent, on les sépare  en 80% pour l'apprentissage et 20% pour le test. D'autres pourcentages courants sont 67-33 ou 50-50.\n",
    "On   utilise la  fonction `train_test_split` du package `sklearn.model_selection` pour séparer aléatoirement les données. \n",
    "\n",
    "Même si le split est aléatoire, on souhaite que les deux échantillons soient tous les deux représentatifs du problème. En particulier, on voudrait qu'ils contiennent le même pourcentage de labels 0 et 1, ce qui est notamment important quand  les labels ne sont pas équilibrés (pas 50-50).\n",
    "\n",
    "Ici, la proportion de 1, individus qui sont dans la catégorie `>50K` est d'environ 0.25 (le vérifier!).\n",
    "\n",
    "Si le taux n'est pas très élevé, une coupe totalement aléatoire des données risque de produire des sous-échantillons où l'un des deux ne contient que peu de labels qui valent 1. Cela peut être problématique pour l'apprentissage du modèle comme pour l'évaluation de la méthode.\n",
    "\n",
    "En pratique, dans des problèmes de classification, on force alors la même répartition des labels dans les deux échantillons TRAIN et TEST, via l'option `stratify`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3ce0cd-5728-475e-83f0-38b8a47d0de7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question\n",
    "\n",
    "Utiliser la fonction `train_test_split` du module `scikit-learn` pour créer à partir de nos données `quantitative_norm`, `qualitative_enc` et `labels` les 4 variables suivantes: \n",
    "- `x_train` et `y_train` qui contiennent 80% du jeu de données pour l'apprentissage du modèle de régression logistique \n",
    "- `x_test` et `y_test` qui contiennent les 20% restants pour le test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155ff94-319a-424f-8210-aa4482b13a28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364ecb2-df56-467f-aa85-a59da6fbea65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Régression logistique\n",
    "\n",
    "Le but de ce TP est de fitter un modèle de régression logistique à ces données afin de prédire le label (`<=50K` ou `>50K`) pour des  nouveaux individus à partir de leurs caractéristiques. Nous commençons par le modèle simple qui prend en compte toutes les variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2a58c-6df9-4f9f-8ae4-7c0cf69bfa9f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question \n",
    "\n",
    "- Utiliser la fonction `LogisticRegression` de `sklearn.linear_model` pour effectuer une régression logistique simple (i.e. sans pénalisation). Fitter le modèle sur les données `train`. Visualiser les coefficients estimés (intercept et variables).\n",
    "- Puis sur le jeu `test`, calculer les probabilités que `>50K` pour chaque individu (`predict_proba()`) et les prédictions (`predict()`) que l'on obtient par seuillage des probabilités au seuil de $1/2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3360062f-58db-4707-818d-65bb78e0fce5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d725b-a51c-414d-b217-2afbc962f5ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Mesures de qualité du classifieur \n",
    "\n",
    "Le module `metrics` de `scikit-learn` contient plusieurs fonctions qui permettent d'évaluer la qualité des prédictions d'un modèle. Ces métriques sont détaillées dans les sections sur les métriques de classification, les métriques de classement multi-label, les métriques de régression et les métriques de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2835f5-2761-4b73-bbbf-0bf4a36a9eaa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "print(dir(metrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e02d6e1-9b21-4bc0-a265-7859b4d48272",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question `score`  \n",
    "\n",
    "Le taux de précision (_accuracy score_) est la mesure de performance qui représente la proportion d'échantillons correctement classés par le modèle parmi l'ensemble total des échantillons, c'est-à-dire $(TP+TN)/n$. Ce taux est un pourcentage, où une valeur de 100% signifie que toutes les prédictions du modèle sont correctes.\n",
    "\n",
    "- Implémenter à la main (en `numpy`) le calcul de ce score pour les données de test,\n",
    "- Obtenir ce score via l'appel de `accuracy_score` du module `sklearn.metrics` et via la méthode `score` de l'objet `model`\n",
    "- Comparer ce score à un estimateur trivial qui consiste à toujours renvoyer la classe la plus fréquente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0620284-c541-42f9-8d67-ed31d2bfdbe4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64330509-5962-4b0a-8964-b09a03a9cb44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question: matrice de confusion, recall, precision\n",
    "\n",
    "La matrice de confusion permet de visualiser les performances du modèle en termes de prédictions correctes (vrais positifs et vrais négatifs) et d'erreurs de prédiction (faux positifs et faux négatifs). La représentation classique se fait sous forme d'une matrice où les nombres sur la diagonale sont liés aux prédictions correctes, tandis que les nombres hors de la diagonale sont liés aux prédictions incorrectes. Plus précisément: \n",
    "\n",
    "- le coin inférieur droit représente les **vrais positifs** (TP): les personnes de la classe `>50K` prédites comme telles par le modèle \n",
    "- le coin supérieur gauche représente les **vrais négatifs** (TN): les personnes de la classe `<=50K` prédites comme telles par le modèle \n",
    "- le coin supérieur droit représente les **faux positifs** (FP): les personnes de la classe `<=50K` prédites comme étant `>50K`\n",
    "- le coin inférieur gauche représente les **faux négatifs** (FN): les personnes de la classe `>50K` prédites comme étant `<=50K`\n",
    "\n",
    "**Attention**:\n",
    "Dans la page wikipedia et dans les slides la matrice est \"inversée\": les vrais positifs sont en haut à gauche... \n",
    "\n",
    "A partir de ces données on peut construire 2 métriques: \n",
    "- le **recall** est défini par TP/(TP+FN): il mesure la capacité du classifieur à identifier tous les exemples positifs réels, c'est-à-dire sa sensibilité ou son taux de détection. Dans notre cadre, le recall est le taux d'individus identifiés comme `>50K` par le classifeur parmi tous les individus avec `>50K` dans `y_test`.\n",
    "- la **precision** est définie par TP/(TP+FP): elle mesure la capacité du classifieur à ne prédire positif que lorsque la prédiction est vraiment correcte, c'est-à-dire sa spécificité.  P. ex.  le classifieur qui prédit systématiquement `>50K` pour tous les individus a un  recall  de 100%, mais sa precision est de 0%. Donc, on veut que les deux scores, le recall et la precision, d'un classifieur soient élevés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844f839-eb19-4152-b3d0-50fdb6b48a63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Tracer la matrice de confusion en utilisant `ConfusionMatrixDisplay` et donner les scores `recall` et `precision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42de42-0315-4813-8db0-709a0428e7ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bfa766-8b94-41bb-8a92-ec3529bda985",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question: courbe ROC, AUC\n",
    "\n",
    "Les valeurs prédites `y_predicted` ci-dessus ont été obtenues en seuillant les probabilités de `>50K` calculées par le modèle de régression logistique avec le seuil par defaut $t=1/2$.  Or, en variant le seuil $t$ il est possible que le classifieur associé ait des meilleurs scores.\n",
    "\n",
    "Avec le seuil $t=0$, le classifieur prédit `<=50K` systématiquement  et on a TP = 0 et FP = 0. En revanche, avec un seuil de $t=1$, le classifieur prédit toujours `>50K`  et donc on a TP = 1 et FP = 1. La courbe ROC représente les points (FP, TP) pour tout seuil $t\\in[0,1]$. \n",
    "\n",
    "Si la courbe est élevée,  le modèle a une très bonne  performance quelque soit le seuil. Cela est mesuré par l'aire sous la courbe (AUC). L'AUC permet de comparer deux modèles, ou un modèle sur des jeux de données différentes, comme les données train et test.\n",
    "\n",
    "Utiliser la fonction `RocCurveDisplay`  pour tracer la courbe ROC de notre modèle  pour les données train et les données test. Sur quelles données on observe une meilleure performance ? Expliquer pourquoi.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ca338-9edb-46ea-89f5-eb40babf7ebb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badcd5fe-6213-4b93-a6e2-c64a2b1eea5a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Question: courbe precision/recall, AP\n",
    "\n",
    "Un autre outil de comparaison est la courbe precision/recall qui représente les points des coordonnées (recall, precision) pour tout seuil $t\\in[0,1]$. L'interprétation est la même : plus la courbe est élevée (ou plus l'aire sous la courbe est grande), mieux c'est.\n",
    "\n",
    "Noter que contrairement à la courbe ROC, la courbe precision/recall n'est pas toujours monotone.\n",
    "\n",
    "Tracer la courbe precision/recall de notre modèle sur les données test en utilisant la fonction `PrecisionRecallDisplay`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a97b0-ba6a-4c66-bc96-272c3723b4ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "to_fill"
    ]
   },
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455942b-9641-44fe-93a1-1a6feb3023bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Régression logistique avec pénalisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ec628-f40a-4496-ada3-c6d56f9ab959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Régression ridge\n",
    "\n",
    "Dans cette partie, nous allons utiliser une pénalité en norme 2 (dite $\\ell_2$ ou Ridge) sur les coefficients de la régression. Cette pénalité Ridge a tendance à \"rétrecir\" les valeurs des coefficients (on parle de shrinkage) et régularise la solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8edc1e-1cd6-4ffa-ac0c-54a290bb5aa5",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_2$ sur les données d'apprentissage. On utilisera toujours la fonction `LogisticRegression`avec  l'option `penalty='l2'`. On utilisera une pénalité assez forte, p. ex. `C=0.01`. \n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité (visualisez l'effet de shrinkage). \n",
    "- Évaluer les performances du classifieur sur les données de test.\n",
    "- Pour les deux classifieurs (avec et sans pénalisation $\\ell_2$), comparer les courbes de score sur les données `test` ainsi que les performances en termes de courbes (ROC ou precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d11b85-3249-49a3-9aba-22d16b326a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01430d-5a6d-45af-b729-3c5dc4d4787c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Régression lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9bb08-8431-4138-92bc-9f599bb79729",
   "metadata": {},
   "source": [
    "\n",
    "Dans cette partie, on va mettre en oeuvre une pénalité $\\ell_1$ dans la régression logistique. Cela va nous permettre de faire de la sélection de variables : les variables peu informatives pour la prédiction ne seront plus du tout utilisées (coefficient $w_i$ estimé à 0). On espère ainsi améliorer les performances de notre prédicteur. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa685ae-cdf4-4b5b-a915-1658bdd649d1",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_1$ sur les données d'apprentissage (avec l'option le `solver='liblinear'`). On utilisera une pénalité assez forte (ex $C=0.01$). \n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité et avec pénalité $\\ell_2$ (visualiser l'effet d'annulation des coefficients). \n",
    "- Pour l'ensemble des classifieurs (avec et sans pénalisation $\\ell_1$ et $\\ell_2$), comparer les performances en termes de courbes (ROC et precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaef890-7ad0-4e2e-9c54-42fa5e6d04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réponse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
