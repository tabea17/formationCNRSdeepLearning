{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff439a24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prédiction de consommation électrique  avec `scikit-learn` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c7627-c31c-43b0-9888-8d3e29c886c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Dans ce TP, nous allons étudier la consommation électrique en France métropole entre janvier 2016 et mai 2023, à partir des données provenant du réseau de transport d’électricité français (RTE), disponibles sur le site https://analysesetdonnees.rte-france.com/.\n",
    "\n",
    "Le fichier `data/data.csv` (ou `data/data.pkl`) a été obtenu en agrégeant les données contenues dans les fichiers suivants : \n",
    "- `rte.csv` : consommation d'électricité relevée toutes les demi-heures\n",
    "- `calendar.csv` : informations sur le temps\n",
    "- `meteo.csv` : informations liées à la météo, notamment l'emplacement des stations et les différents relevés (température, nébulosité, humidité, vitesse du vent, précipitation), pris toutes les 3 heures. \n",
    "\n",
    "Pour les détails sur la génération de ce fichier, voir le notebook `day2_preparation_data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9f17a-c471-4870-826f-07555d22ddd4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# EX 1 : Import des données\n",
    "\n",
    "Les données agrégées se trouvent dans le fichier `data/data.pkl`.\n",
    "\n",
    "- Importer les données sous forme d'un `DataFrame` pandas que l'on nomme `data`\n",
    "- Visualiser les premières et dernières lignes des données\n",
    "- Afficher la taille du `DataFrame`\n",
    "- Afficher les noms des premières 17 colonnes et ensuite des colonnes restantes\n",
    "- Faire de même pour le type de données présentes dans ces colonnes\n",
    "- Vérifier que le `DataFrame`ne contient pas de données manquantes\n",
    "- Convertir les variables `Mois`, `Jour`, `JourFerieType` en variables dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4011f98-958c-43a4-b8db-1bbf25ef6033",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e52c796a-075c-4cdb-bb78-a93fe4983c79",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Analyse des données\n",
    "\n",
    "Observons le tableau `data` des données agregées.\n",
    "\n",
    "## EX 2 : Saisonnalité\n",
    "\n",
    "Afficher le graphe des consommations éléctriques en fonction du temps. Observez-vous une saisonnalité ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775daca1-d6fd-4911-8739-b3e377e0463b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c84bd31a-a3ac-499f-844a-3af74492ddd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Profils de consommation\n",
    "\n",
    "En utilisant les fonctions `groupby` et `agg` de pandas on peut regrouper les données en faisant la moyenne des consommations par mois, par jour, ou par demi-heure, de sorte à obtenir des profils type de consommation pour une année/un mois/une semaine/un jour.\n",
    "\n",
    "Par exemple, la moyenne des consommations à l'échelle des années montre des plus fortes consommations pendant les mois d'hiver et plus faibles pendant l'été. :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c3d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the annual average consumption \n",
    "monthly_avg = data.groupby([\"DemiHeure\", \"Mois\", \"MJour\"]).agg(Consommation=(\"Consommation\", \"mean\"), DateTime =(\"DateTime\" ,\"first\")).reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.lineplot(x=\"DateTime\", y=\"Consommation\", data=monthly_avg, hue=\"Mois\", legend=False) \n",
    "plt.xlabel(\"\") \n",
    "plt.ylabel(\"\")\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b'))\n",
    "plt.title(\"Average Consumption\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa061352",
   "metadata": {},
   "source": [
    "### Exercice 3\n",
    "\n",
    "- Afficher le profil de consommation hebdomadaire et journalier\n",
    "- Afficher le profil de consommation journalier par jour de la semaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d83559-f60f-4c8e-b6e1-76860cebe3ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8ba5e27-2ec7-41bc-a016-793f70087096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Comportements particuliers\n",
    "\n",
    "Il est possible de mettre en évidence des petites anomalies dues au jours fériés ou au printemps 2020. \n",
    "\n",
    "La consommation du mois de mai 2017, en mettant en évidence les jours féries (le 1er mai 2017 était un lundi) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the consumption for May 2017\n",
    "may_2017_data = data[(data['Annee'] == 2017) & (data['Mois'] == 'mai')]\n",
    "may_2017_ferie = may_2017_data[may_2017_data['JourFerie'] == 1]\n",
    "may_2017_ferie['MJour'].unique()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(may_2017_data['DateTime'], may_2017_data['Consommation'], color='blue')\n",
    "for i in may_2017_ferie['MJour'].unique():\n",
    "    plt.plot(may_2017_ferie[may_2017_ferie['MJour'] == i]['DateTime'], may_2017_ferie[may_2017_ferie['MJour'] == i]['Consommation'], color='black')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.title('Consumption in May 2017')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871aa4c9",
   "metadata": {},
   "source": [
    "#### EX 4\n",
    "- Comparer la consommation moyenne de l'année 2020 à celle du reste des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747613a-dfb8-4a36-9a5b-62ae8757c21f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e68f0e2-f668-42bc-a8c2-798a6a3ba4b0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Modèles : entraînement, prédiction et erreur "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb3a2c3-041b-487b-9740-32833b112ea8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## EX 5 : Séparation du jeu de données en train et test\n",
    "\n",
    "Séparer le jeu de données en deux pour obtenir le jeu d'entraînement et le jeu de test tels que \n",
    "\n",
    "- `X_train` et `Y_train` contiennent 50% du jeu de données pour l'apprentissage du modèle  \n",
    "- `X_test` et `Y_test` contiennent les 50% restants pour le test.\n",
    "\n",
    "On pourra utiliser la  fonction `train_test_split` du module `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbdb1bb-ca5f-4093-b2e8-52a421db6dad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787a10e0-57e2-492e-982c-954c7e7439a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Naive predictor\n",
    "\n",
    "Pour pouvoir comparer les différents modèles, nous allons construire un prédicteur naïf qui renvoie la moyenne des consommations (une constante). Ce prédicteur trivial représente le point de départ, à partir duquel nous allons nous améliorer.\n",
    "\n",
    "Pour évaluer les performances des modèles, nous allons calculer l'erreur MAPE et l'erreur RMSE des prédicteurs sur le jeu de test et sur celui de train.\n",
    "\n",
    "Pour rappel, si $\\hat Y_t$ est la valeur prédite par le modèle et $Y_t$ la consommation réalisée :\n",
    "\n",
    "MAPE = **mean absolute percentage error** : c'est une erreur de prédiction *relative*:\n",
    "$${\\displaystyle {\\mbox{MAPE}}={\\frac {1}{n}}\\sum _{t=1}^{n}\\left|{\\frac {Y_{t}-\\hat Y_{t}}{Y_{t}}}\\right|}\n",
    "$$\n",
    "En pratique, on préfère diviser par $|Y_{t}+\\varepsilon|$ pour éviter des division par zéro.\n",
    "\n",
    "RMSE = **root mean squared error** : c'est une erreur de prédiction *absolue*, sensible aux outliers:\n",
    "$${\\displaystyle {\\mbox{RMSE}}=\\sqrt{\\frac 1 n \\sum _{t=1}^{n}\\left(Y_{t}-\\hat Y_{t}\\right)^2}}\n",
    "$$\n",
    "\n",
    "### EX 6\n",
    "\n",
    "- Construire un prédicteur naïf qui renvoie la moyenne des consommations sur le jeu de train et sur celui de test.\n",
    "- Calculer les erreurs MAPE et RMSE de ce prédicteur naïf (sur le jeu de train et le jeu de test)\n",
    "- Stocker ces erreurs dans un DataFrame `df_errors`, qui va servir à stocker les erreurs des différents prédicteurs que nous allons mettre en oeuvre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6165dac-ba39-4ef0-9e8d-a2d4d3d74dbd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "On pourra utiliser les fonctions `mean_absolute_percentage_error` et `root_mean_squared_error` du module `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb71cb-c669-409f-8985-ebd73c7e7f90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2837339-956b-49da-a26b-fb54a702251e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c6931a-7eae-4f1c-9a95-977c5c3fe329",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Tous les modèles de régression de la librairie scikit-learn ont une méthode `fit` qui permet d'entraîner le modèle et une méthode `predict` qui calcule la valeur prédite.\n",
    "\n",
    "Nous allons comparer plusieurs méthodes et pour cela il sera utile de définir une fonction `fit_and_predict_error` qui, étant donné un modèle `model`, et un jeu de données `x_train`,`y_train`, `x_test`, `y_test`, entraîne le modèle en appelant la méthode `fit` du modèle sur le jeu d'entraînement, calcule la prédiction en appelant la méthode `predict` du modèle à la fois sur le jeu de test et sur le jeu de train, et calcule les erreurs MAPE et RMSE sur les 2 jeux.\n",
    "\n",
    "Cette fonction renvoie un dictionnaire contenant les prédictions calculées, `train` et `test`, et les erreurs MAPE et RMSE pour les jeux de train et de test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8845c284-3f07-4c1c-883b-6504424dd6b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Pour stocker l'information sur les erreurs dans le tableau des erreurs `df_errors`, nous définissons également une fonction qui permet de le faire rapidement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629538f9-ab59-4781-81dd-9e117a4419e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_error(model_out, model_name, df):\n",
    "    return df._append({'Model' : model_name, \n",
    "                  'MAPE test' : model_out['mape_test'], \n",
    "                  'RMSE test' : model_out['rmse_test'], \n",
    "                  'MAPE train' : model_out['mape_train'], \n",
    "                  'RMSE train' : model_out['rmse_train'], \n",
    "                  'CPU time' : model_out['time']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3894f7-b9a9-4f2c-af01-4b78e1eaa8b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Regression linéaire\n",
    "\n",
    "Nous allons en première approche utiliser des modèles linéaires pour apprendre et prévoir les données de consommation.  \n",
    "\n",
    "A l'aide de la librairie `sklearn.linear_model`, nous pouvons mettre en place plusieurs modèles de regression linéaire pour prédire la variable `Consommation` en fonction de différentes caractéristiques.\n",
    "\n",
    "Un premier modèle linéaire (modèle 1) utilise les données de température comme covariables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd136b-5dc2-402d-8f4f-b4ed81b16d2b",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2997f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Model\n",
    "linear_pipe = make_pipeline(StandardScaler(), LinearRegression(fit_intercept=True))\n",
    "\n",
    "# Model 1 : Linear regression on temperature\n",
    "linear_temp = fit_and_predict_error(linear_pipe, X_train[['Temperature']], Y_train, X_test[['Temperature']], Y_test)\n",
    "complete_linear_temp = linear_pipe.predict(data[['Temperature']])\n",
    "df_errors = add_error(linear_temp, 'Linear on Temperature', df_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f447d554",
   "metadata": {},
   "source": [
    "### EX 7\n",
    "\n",
    "Mettre en place plusieurs modèles de regression linéaire pour prédire la variable `Consommation` en fonction des caractéristiques :\n",
    "\n",
    "    2. Données méteo\n",
    "    3. Données méteo et `PositionDansAnnee`, `JourFerie`, `DemiHeure`\n",
    "    4. Les caractéristiques ['Temperature', 'Nebulosity', 'Humidity', 'WindSpeed', 'Precipitation', 'PositionDansAnnee', 'DemiHeure', 'JourFerie',  'Vacances', 'MJour', 'Annee', 'is.2020']\n",
    "    5. Toutes les caractéristiques\n",
    "\n",
    "- Lequel de ces modèles donne les meilleurs résultats ? \n",
    "\n",
    "- Afficher, pour les modèles naïf, 1, 4 et 5, la consommation réelle et la consommation prédite sur les données complètes, sur un même graphe (il y a donc 4 graphes, un par modèle). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9aed08-ed07-4499-90a4-f5a3579d28d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfbdf8a1-f7d8-443d-9204-f3532a48364a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Nous allons dorenavant travailler uniquement sur deux ensembles de caractéristiques :\n",
    "- Les caractéristiques réduites  :  ['Temperature', 'Nebulosity', 'Humidity', 'WindSpeed', 'Precipitation', 'PositionDansAnnee', 'DemiHeure', 'JourFerie', 'Vacances', 'MJour', 'Annee', 'is.2020']\n",
    "- Les caractéristiques complètes\n",
    "\n",
    "### EX 8\n",
    "\n",
    "Créer deux jeux de données d'entrainement (et de test) `X_train_S` et `X_train_L` (respectivement `X_test_S` et `X_test_L`) correspondants à ces deux ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f658244-51ca-4754-80c2-4a7b4d798abf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78837329-6e17-4720-bb7d-4ceb59677d35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Lasso and Ridge regression\n",
    "\n",
    "Essayons d'améliorer les performances en ajoutant une pénalité $\\ell_1$ et une pénalité $\\ell_2$ à la regression linéaire. \n",
    "\n",
    "### EX 9 :  Lasso et Ridge\n",
    "\n",
    "A l'aide des modules `LassoCV` et `RidgeCV` mettre en place deux estimateurs sur les données réduites et sur les données complètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714321b-5510-4003-83ce-746d68630096",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a30390bf-26ea-48a4-9e84-fa1cd6636233",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Arbres de décision -- CART\n",
    "\n",
    "Les *Classification and Regression Trees* (CART) permettent de prédire la consommation en suivant des simples règles de décision qui dépendent des caractéristiques observées.\n",
    "\n",
    "Le choix du nombres et du type de caractéristiques utilisées pour construire un arbre, ainsi que sa profondeur influent énormément sur le résultat, les paramètres à indiquer dans sa construction sont à choisir attentivement. \n",
    "\n",
    "A noter que dans un contexte d'arbres de décision la normalisation des caractéristiques n'est pas nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8d5ac1-6472-4ec9-870f-78e0d854d9bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### EX 10 : Arbre par défaut\n",
    "\n",
    "Utiliser la classe `DecisionTreeRegressor` du module `sklearn.tree` pour construire un estimateur basé sur un arbre de décision.\n",
    "\n",
    "- Utiliser initialement les paramètres par défaut.\n",
    "\n",
    "- Entraîner le modèle sur les données réduites et sur les données complètes.\n",
    "\n",
    "- Afficher les erreurs. Que remarque-t-on sur les erreurs calculées sur le train set ? Comment le justifier ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e5675-2c85-44e9-84b9-8aeb7deafad3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbf3f763-7ba2-4441-b90c-61a2431b67df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Réduction de profondeur\n",
    "\n",
    "Retrouver dans la documentation les valeurs par défaut des paramètres d'un arbre de régression suivants :\n",
    "\n",
    "- max_depth\n",
    "- min_samples_leaf\n",
    "- max_features\n",
    "  \n",
    "Quelle est la profondeur de l'arbre construit par défaut ? \n",
    "\n",
    "Construire un arbre de profondeur 10 et comparer les erreurs avec les modèles précédents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9648fdd-ef1a-4e6e-8b27-e7f1968ba87c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8864010-7d34-4dae-a4d4-7019f39866d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Visualisation\n",
    "\n",
    "- Construire un arbre de profondeur 4 et l'entraîner sur les données complètes\n",
    "- A l'aide de la fonction `plot_tree` du module `tree` de sklearn, visualiser l'arbre de décision.\n",
    "\n",
    "- Quelles sont les caractéristiques sur lesquelles sont fait les splits de cet arbre ? \n",
    "- Combien de noeuds a cet arbre ? Combien d'observations y a-t-il dans chaque noeud ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4530f2f-2f8c-4a7f-ada3-2ed140caf50c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eef8d2e6-a924-4181-b8ba-66af04ed3664",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf025c8-b1c4-416e-a1f9-d97e81a76c51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Le bagging (ou agrégation Bootstrap) est un type d'apprentissage d'ensemble dans lequel plusieurs modèles de base sont entrainés indépendamment et en parallèle sur différents sous-ensembles des données d'entraînement. Dans le bagging, les étapes d'entraînement et de prédiction sont précédées par une étape de bootstrap.\n",
    "\n",
    "1. **Échantillonnage bootstrap** : Dans l'échantillonnage bootstrap, $K$ sous-ensembles aléatoires des données originales sont échantillonnés avec remplacement. Cette étape garantit que les modèles de base sont formés sur divers sous-ensembles des données, car certains échantillons peuvent apparaître plusieurs fois dans le nouveau sous-ensemble, tandis que d'autres peuvent être omis. Cela réduit les risques de surajustement et améliore la précision du modèle.\n",
    "\n",
    "2. **Entraînement sur les modèles de base** : Après la première étape d'échantillonnage bootstrap, le modèle de base (arbres de décision, SVM...) est entraîné indépendamment sur chaque sous-ensemble de données bootstrap différent. Ces modèles de base sont généralement dits « faibles » car ils peuvent ne pas être très précis à eux seuls. \n",
    "\n",
    "3. **Agrégation** : Une fois que tous les modèles de base ont été entraînés, ils sont utilisé pour faire chacun une prédiction sur les données de test. Dans les modèles de classification, la prédiction finale est effectuée en agrégeant les prédictions des modèles de base en utilisant le vote majoritaire. Dans les modèles de régression, la prédiction finale est obtenue en faisant la moyenne des prédictions des modèles de base.\n",
    "\n",
    "**Évaluation Out-of-Bag (OOB)** : Dans l'étape d'échantillonage, certaines observations sont exclues de l'échantillon boostrap. Ces observations *out-of-bag* peuvent être utilisées pour évaluer les performances du modèle.\n",
    "\n",
    "![Classificateur par agrégation bootstrap](img/Bagging-classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfedea9-4cf4-43ff-a86f-ff1b1c7b5bdf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Les arbres de décision vus au point précédent sont extrêmement dépendant du training set. Les modèles de bagging, grâce à l'étape d'échantillonnage aléatoire, permettent de réduire cette sensibilité aux données d'entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e3c1ad-af63-42e4-8c92-ac3f3663547d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Bagging à la main\n",
    "\n",
    "Ecrire un bagging à la main de taille `K=100` *sur les données réduites* en suivant les étapes suivantes :\n",
    "\n",
    "1. Créer K échantillons boostrap de même taille que l'échantillon d'entraînement :\n",
    "    - Utiliser la fonction `np.random.choice` avec paramètre `replace=True` pour générer un tableau d'indices `indexes` de même taille que l'échantillon original\n",
    "    - Un échantillon booststrap est donné par `X_train_S[indexes]`\n",
    "2. Entraîner un arbre de décision simple (peu profond, avec un nombre d'observations minimal par feuille, ...) sur chaque échantillon bootstrap\n",
    "3. Calculer la valeur prédite par bagging :\n",
    "      - Calculer la prédiction de chaque arbre\n",
    "      - Renvoyer la moyenne des valeurs prédites par les K arbres\n",
    "4. Calculer et afficher les erreurs MAPE et RMSE *out-of-bag*. \n",
    "\n",
    "Une fois le modèle sur les données réduites maîtrisé, il est possible de passer sur les données complètes, tout en tenant compte des temps de calculs attendus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77d23d7-3ace-4df0-a96e-af73fec79e23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abc12333-935d-45c0-9986-4bcbb7845e64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Bagging de sklearn\n",
    "\n",
    "- Comparer avec un bagging construit en utilisant la fonction `BaggingRegressor` du module `sklearn.ensemble`.\n",
    "- Afficher le score *out-of-bag* de ce prédicteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbe83e0-fe0a-4edd-80c3-e6ce05edf321",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc9ac300-9e2e-4098-bce9-db69814ca55e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Random forest\n",
    "\n",
    "Le forêts aléatoires sont une extension du bagging pour les arbres de décision qui permet de réduire davantage la variance du modèle, en ajoutant un caractère aléatoire lors de la création des arbres.\n",
    "\n",
    "Pendant la construction d'un arbre, au lieu de rechercher la caractéristique la plus importante lors de la division d'un nœud, on recherche la meilleure caractéristique **parmi un sous-ensemble aléatoire de caractéristiques**. Il y a donc deux niveaux *random* dans ce modèle (d'où le nom \"random forest\"): le random du bootstrap et la sélection random des caractéristiques. Le premier réduit la dépendance au training set, le deuxième réduit la corrélation entre les arbres de la forêt. Il en résulte une grande diversité qui se traduit généralement par un meilleur modèle.\n",
    "\n",
    "### Exercice\n",
    "\n",
    "- A l'aide de la classe `RandomForestRegressor` du module `sklearn.ensemble`, entraîner une foret aléatoire avec paramètres par défaut sur les données réduites et comparer les erreurs avec les autres modèles. Afficher également le score *out-of-bag* de ce prédicteur.  \n",
    "- Visualiser dans un graphe l'évolution des erreurs en fonction du nombre d'arbres de la forêt.\n",
    "- Les résultats théoriques montrent que le bon nombre de caractéristiques à tenir en compte lors d'un split est soit de l'ordre de la racine carré du nombre total des caractéristiques, soit de son logarithme. Entraîner une foret aléatoire avec un nombre d'arbres choisi sur la base de la visualisation du point précédent, et un nombre de caractéristiques de l'ordre de la racine carré du nombre total de caractéristiques. Afficher également le score *out-of-bag* de ce prédicteur.  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f30ee6-00e1-4032-ab17-ea0216c8d9df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "059c115c-e34f-41bf-a972-b9ef5e3331f6",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Extra trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d06b2-2f23-4d00-a1c6-7a3cf818dab0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Proche des forêts aléatoires, les Extra Trees, dit aussi Extremely Randomized Trees, ajoutent une couche supplémentaire d'aléatoire au forêts, en choisissant le split d'une caractéristique au hasard, au lieu qu'en sélectionnant le meilleur split. Cela réduit par ailleurs de façon considérable le temps de calcul.\n",
    "\n",
    "### Exercice\n",
    "\n",
    "- Combiner la classe `ExtraTreeRegressor` et `BaggingRegressor` pour obtenir une forêt d'arbres extrêmement aléatoires.  \n",
    "- Entraîner, prédire et afficher le score *out-of-bag* d'abord sur les données réduites, et ensuite sur les données complètes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b85765a-57dd-46d5-bc03-24b90519d73f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6f4243a-2198-426f-a007-ea15e6bbb5a9",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Nous avons vu que les méthodes de Bagging permettent de construire un prédicteur robuste à partir de plusieurs prédicteurs faibles. Les prédicteurs faibles peuvent être construits en parallèle, c'est-à-dire que la construction de chacun d'entre eux peut être faite indépendamment des autres.\n",
    "\n",
    "Une autre technique d'ensemble, différente du Bagging, consiste à générer les prédicteurs de façon séquentielle, en améliorant à chaque itération le nouveau modèle par rapport au précédent. C'est la technique du **Boosting**.\n",
    "\n",
    "Tout d'abord, un modèle est construit à partir des données d'apprentissage. Ensuite, un deuxième modèle est construit pour tenter de corriger les erreurs présentes dans le premier modèle. Cette procédure se poursuit et des modèles sont ajoutés jusqu'à ce que l'ensemble des données d'apprentissage soit prédit correctement ou que le nombre maximum de modèles soit atteint. \n",
    "\n",
    "Dans toutes les techniques de Boosting, des poids sont associés aux données d'apprentissage, et à chaque itération ces poids sont réajustés, de façon à augmenter le poids des données mal prédites  et réduire celui des données correctement prédites. Ceci permet au modèle suivant de se \"concentrer\" sur les mauvaise prédictions et de s'améliorer.\n",
    "\n",
    "![Boosting classifier](img/Boosting-classifier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b987e-3861-4bdd-a795-1cbf8c6e609e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Il existe plusieurs algorithmes de Boosting, parmi les plus connus nous pouvons citer : \n",
    "- **Gradient Boosting** :\n",
    "    - Chaque nouveau modèle est entraîné pour minimiser la fonction de perte (erreur quadratique moyenne ou cross-entropie) du modèle précédent à l'aide d'une descente de gradient.\n",
    "    - À chaque itération, l'algorithme calcule le gradient de la fonction de perte par rapport aux prédictions de l'ensemble actuel, puis entraîne un nouveau modèle faible pour minimiser ce gradient.\n",
    "    - Les prédictions du nouveau modèle sont ensuite ajoutées à l'ensemble, et le processus est répété jusqu'à ce qu'un critère d'arrêt soit rempli.\n",
    "- **AdaBoost** -- Adaptative Boosting\n",
    "- **XGBoost** -- eXtreme Gradient Boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be90f8a-68bf-4baa-bd2d-079abf85d304",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice\n",
    "\n",
    "Utiliser le module `GradientBoostingRegressor` pour prédire la consommation via une regression par Gradient Boosting avec un learning rate égal à 0.7, un nombre d'estimateurs égal à 100 et une profondeur de l'arbre égale à 10. \n",
    "\n",
    "Si vous en avez le temps, faites de la validation croisée sur le learning rate et le nombre d'estimateurs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df3aa31-40fe-423c-9eae-b4035ede05a2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40459a5-4c63-4863-9a7b-5026c7e5fc32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Feature importance/importance des variables\n",
    "\n",
    "L'importance des variables indique dans quelle mesure chaque caractéristique contribue à la prédiction du modèle. Une première idée de l'importance des variables, par exemple, peut être donnée par la correlation entre chaque variable et la variable à prédire. Selon le modèle, il existe d'autres mesures qui peuvent aider à affiner la compréhension de quelles variables contribuent le plus à la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07cff7b-f098-41ed-ba59-c7768af9925d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Regressions linéaires\n",
    "\n",
    "Combien de caractéristiques sont retenues dans les modèles Lasso ? \n",
    "\n",
    "Commenter au vu des erreurs obtenues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71361bc5-e651-4824-85a7-2f4b9c4d81bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adfc4fb2-9cd9-4f09-b60b-08a2c9f2450c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Exercice : Modèles d'ensembles \n",
    "Pour les arbres de décisions, les forêts aléatoires ou le gradient boosting, l'importance d'une variable peut être calculée comme la réduction totale du critère apportée par cette variable.\n",
    "\n",
    "Afficher dans un histogramme à barres horizontales (`barh`) les 10 variables le plus importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ce29a-f6bb-4ef2-996b-aade027c0239",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bce0b70a-f490-49fd-b110-268a53fbb994",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Approche LSTM\n",
    "\n",
    "Le but est d'illustrer l'utilisation d'un réseau de neurones récurrents (RNN) pour la prédiction de la consommation. Pour simplifier on oublie les autres variables et on cherche une relation entre les consommations passées (sur une période de temps à définir) et la consommation à venir (ou les consommations). \n",
    "\n",
    "On note $(y_k)_{k = 1, \\dots,n}$ la série des consommations aux instants $t_k$ (les différentes demi-heures). \n",
    "\n",
    "Fixons $\\tau \\ge 1$ une taille de fenêtre de temps et $k \\ge \\tau$. On cherche une relation entre la consommation $y_{k+1}$ à l'instant $t_{k+1}$ et les consommations passées $y_{k-\\tau}, \\dots, y_k$ c'est à dire construire une prédiction $\\hat y_{k+1}$ comme fonction de $y_{k}, \\dots, y_{k-\\tau}$\n",
    "\\begin{equation*}\n",
    "    \\hat y_{k+1} = g^{(\\tau)} \\big( y_{k}, \\dots, y_{k-\\tau} \\big).\n",
    "\\end{equation*}\n",
    "L'idée d'un RNN est d'introduire un état caché $h$ vecteur de $\\mathbf{R}^d$ et une fonction paramétrique $f_\\theta$ tels que,\n",
    "\\begin{equation*}\n",
    "    \\forall j \\ge {k-\\tau}, \\quad h_j = f_\\theta\\big(h_{j-1}, y_j \\big), \n",
    "\\end{equation*}\n",
    "avec $h_{k-\\tau-1}$ fixé, par exemple un vecteur nul. Si on \"dérécursive\" cette relation on obtient \n",
    "\\begin{align*}\n",
    "    h_{k-\\tau} &= f_\\theta\\big(h_{k-\\tau-1}, y_{k-\\tau} \\big) \\\\\n",
    "    h_{k-\\tau+1} &= f_\\theta\\big(f_\\theta\\big(h_{k-\\tau-1}, y_{k-\\tau} \\big), y_{k-\\tau+1} \\big) \\\\\n",
    "    &\\dots \\\\\n",
    "    h_k & = f_\\theta\\big(f_\\theta\\big(\\dots, y_{k-1} \\big), y_{k} \\big)\n",
    "\\end{align*}\n",
    "Ainsi $h_k$ est une fonction de $\\big(y_{k-\\tau},\\dots,y_k\\big)$. Comme prédiction $\\hat y_{k+1}$ on choisit alors une application linéaire de $h_k$\n",
    "\\begin{equation*}\n",
    "    \\hat y_{k+1} = A h_k + \\beta\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45ad46-2b8f-4351-aa5f-138e8b5011d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b324a5-de0e-433b-b463-b247db45c960",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Préparation des données \n",
    "\n",
    "Il est important de normaliser les données puis de créer un dataset que l'on va séparer en 2 jeux disjoints: `train set` et `test set`.  \n",
    "\n",
    "1. Normaliser les données et créer un tensor `pytorch`\n",
    "2. Fixer une taille de fenêtre $\\tau$ (par exemple $\\tau = 92$ pour avoir 2 jours d'historique) et découper la série temporelle pour créer un dataset\n",
    "\\begin{equation*}\n",
    "    \\mathcal{D} = \\big\\{ \\forall k \\ge \\tau+1, \\quad \\big( (y_{k-\\tau},\\dots,y_k), y_{k+1} \\big) \\big\\}\n",
    "\\end{equation*}\n",
    "\n",
    "3. Découper le dataset $\\mathcal{D}$ en des données d'entrainement et des donnés de test.\n",
    "4. Créer un `dataloader` `pytorch` pour charger les données par paquet (batch) de taille 32.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80028a-1a83-4bdb-b6e6-9b7d476f4e2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7850357-7b6d-4920-98f8-a5077a1fc1d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Initialisation de la fonction de prédiction \n",
    "\n",
    "Créer une classe (fille de `nn.Module` de `pytorch`, on verra le détail demain dans le TP3) qui implémente la fonction $g^{(\\tau)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7abb8c-597c-43fb-933e-3eeb0b48e0fa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5084ca49-634a-49f0-8f87-fe0b5c83a588",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Entrainement (apprentissage) de la fonction de prédiction\n",
    "\n",
    "On va parcourir plusieurs fois le `train set` pour apprendre la fonction paramétrique $g^{(\\tau)}$: apprendre c'est à dire résoudre un problème d'optimisation. Le problème ici est de minimiser un critère: par exemple la distance entre la valeur prédite $g^{(\\tau)}(y_k, \\dots, y_{k-\\tau})$ et $y_{k+1}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda77a6-53bd-429a-9587-34a09de620fd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca770a7-4ac5-4b31-a81b-302ef3f57db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Une première vérification\n",
    "\n",
    "Pour se comparer on utilise un estimateur naturel (pour ce problème d'estimation de la consommation) qui consiste à utiliser la consommation actuelle $y_k$ pour prédire la consommation de la demi-heure suivante $y_{k+1}$. \n",
    "\n",
    "On utilise l'objet `transformer` pour revenir dans l'échelle des consommations et on calcule la MAPE pour ces deux estimateurs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01943e23-20fb-475e-a753-b09b07a60643",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f90e726-0cf5-4f1c-9d15-85afd7de737b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Quelques prédictions \n",
    "\n",
    "On fait quelques expériences numériques pour visualiser notre fonction de prédiction basée sur le LSTM entrainé. Attention on fait ici juste une illustration de quelques scénarios, il faudrait une étude plus quantitative pour \"valider\" ou non le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963ba0f-f016-40b0-a507-13f351c8c46c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "aremplir"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
