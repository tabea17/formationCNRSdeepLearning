{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classification binaire avec la régression logistique\n",
    "\n",
    "\n",
    "Dans ce TP, nous allons apprendre à mettre en oeuvre un modèle de régression logistique pour la prédiction de labels binaires $y$.   \n",
    "Rappel : la régression logistique repose sur une modélisation probabiliste, et donc plutôt que de prédire simplement 0/1, \n",
    "on calcule pour  chaque nouvelle observation un  score  entre 0 et 1 qui correspond à un estimateur de la probabilité $\\mathbb P(y=1|x)$. \n",
    " \n",
    "## Données *score*\n",
    "\n",
    "Le tableau *score* contient des données sur la solvabilité d'anciens clients en vue d'un crédit pour l'achat d'un véhicule.  La colonne `Y` indique si le client était solvable ($y=0$) ou pas ($y=1$) et c'est le label à prédire. Les autres colonnes du tableau sont des covariables avec des informations supplémentaires sur les clients. \n",
    "On cherche à construire un modèle qui explique la solvabilité en fonction de ces covariables : on veut donc apprendre la relation précise entre les covariables et le label à prédire.\n",
    "\n",
    "Il n'est pas clair si toutes les covariables sont utiles pour cette tâche, donc dans un second temps, on se posera la question de la sélection des covariables qui servent à prédire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer les données\n",
    "### Charger les données à partir du fichier externe\n",
    "On commence par charger les données stockées dans un fichier *day1_score.csv*. Si le fichier n'est pas trop gros, vous pouvez commencer par ouvrir ce fichier **avec un simple éditeur de texte**, pour identifier sa structure et paramétrer son importation. Vous constaterez que la première ligne indique les noms des variables, le séparateur est `;` et les décimales sont codées par des virgules (et non des points). Sinon, importation avec les paramètres standards et itérations successives pour trouver le bon format ! \n",
    "\n",
    "On peut simplement afficher les premières lignes avec la commande suivante afin de savoir comment paramétrer la fonction d'importation des données à partir du fichier :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -2 'data/day1_score.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Votre chemin vers les données. Par défaut celui du notebook \n",
    "# path_data = '.'\n",
    "path_data = 'data/' \n",
    "\n",
    "# Lecture du fichier csv\n",
    "df = pd.read_csv(os.path.join(path_data, 'day1_score.csv'), sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1\n",
    "\n",
    "- Familiarisez-vous avec les données (à l'aide de `shape`, `head`, `dtypes`, `describe`, `iloc`,`items`). \n",
    "- Séparer les données en 3 dataframes qu'on nomme :\n",
    "\n",
    "    - `defaut` : pour les labels observés (`Y`)\n",
    "    - `quanti`: dataframe avec toutes les covariables quantitatives\n",
    "    - `quali`: dataframe avec toutes les covariables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Le jeu de données contient 10 000 lignes (individus) décrits par 21 variables (colonnes).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "La première variable `Y` est la variable binaire à prédire. Il y a ensuite 10 variables quantitatives (`X1` à `X10`) et 10 variables qualitatives (`X11` à `X20`).\n",
    "\n",
    "Résumé des variables quantitatives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Si on souhaite un résumé incluant les variables qualitatives :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Quelques stats descriptives, quantitatives et qualitatives, met des NaN si incongru.\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Noter que `freq` indique le comptage de la modalité la plus fréquente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Combien de modalités pour chaque variable quali ?\n",
    "for colname, col in df.iloc[:, 11:21].items():  # .items() permet d'itérer sur les noms des colonnes et leurs contenus\n",
    "    print(colname + ':')\n",
    "    print(col.value_counts())\n",
    "    print('-' * 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "Séparons les différents types de variables :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identification selon le type de variable\n",
    "defaut = df['Y'] \n",
    "quanti = df.iloc[:, 1:11]\n",
    "quali = df.iloc[:, 11:21]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation des données quantitatives et codage des qualitatives\n",
    "\n",
    "On ne travaille que très rarement sur des donnnées brutes. \n",
    "\n",
    "Pour des  **variables quantitatives**, il est important des les  **standardiser** (centrer et réduire) afin de les ramener à  des échelles comparables. Cela diminue également les problèmes numériques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Pour une **variable qualitative** ou **catégorielle**, il n'y a en général pas  d'ordre naturel de ses modalités. Donc il ne faut pas bêtement remplacer des catégories *A*, *B*, *C*,... par 1, 2, 3,... Le problème est que dans un modèle de régression, on va calculer par exemple des moyennes sur ces valeurs, mais en général *B* n'est pas la moyenne de *A* et *C* (alors que c'est le cas pour la valeur 2 par rapport à 1 et 3). Il nous faut alors un encodage qui est invariant à l'ordre des modalités. On utilise  le **one-hot encoding** qui consiste à transformer une variable qualitative en plusieurs variables binaires dites **dummies**. Une variable avec K modalités est transformée en K-1 variables binaires. Chacune de ces variables binaires indique pour une catégorie spécifiée si la variable observée est égale à cette catégorie ou pas. Il suffit de K-1 variables binaires pour encoder K catégories, car l'information sur la K-ième catégorie peut être déduite des K-1 autres variables binaires. Donc, si on utilisait K variables binaires, on introduirait des corrélations entre les colonnes, ce qu'il faut éviter dans un cadre de régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2\n",
    "\n",
    "- Standardiser les données quantitatives.\n",
    "- Créer des dummies pour toutes les variables qualitatives avec la fonction `get_dummies` de `pandas` et l'option `drop_first=True`.\n",
    "- Créer un seul dataframe qu'on nommera `df_trav` contenant toutes les covariables transformées (quantitatives et qualitatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reduction des variables quantitatives\n",
    "\n",
    "# Méthode directe \n",
    "quantiN = (quanti-quanti.mean())/quanti.std()\n",
    "\n",
    "## Méthode sophistiquée avec StandardScaler\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "#sc = StandardScaler()\n",
    "#quantiN = pd.DataFrame(sc.fit_transform(quanti), columns=quanti.columns)\n",
    "\n",
    "# Nouveau dataframe avec les var quanti normalisées\n",
    "df_trav = pd.concat([quantiN, quali], axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On transforme les variables qualitatives en dummies\n",
    "df_trav = pd.get_dummies(df_trav, drop_first=True)  # .getdummies() n'agit que sur les var qualitatives et garde les quantitatives\n",
    "# Ne pas oublier 'drop_first=True' sinon on garde K dummies pour chaque variable qui a K modalités ! \n",
    "df_trav.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Noter qu'à présent toutes les variables sont quantitatives (plus besoin de l'option include=\"all\" dans la fonction `describe`)\n",
    "df_trav.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df_trav.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On enregistre les noms des colonnes\n",
    "features_names = df_trav.columns.tolist()\n",
    "print(features_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On peut éventuellement sauvegarder notre jeu de données. Il est prêt à être analysé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Si on veut sauvegarder ce jeu de données dans un fichier\n",
    "\n",
    "#import pickle as pkl\n",
    "\n",
    "## Sauver le jeu de données dans un fichier \n",
    "#with open(os.path.join(path_travail, 'df_dumN.pkl'), 'wb') as f:\n",
    "#    pkl.dump({'features': df_trav, 'labels': defaut}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Échantillons d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On appelle les données : features X et variables à prédire Y \n",
    "X, Y = df_trav, defaut\n",
    "\n",
    "# Si on veut recharger le jeu de données depuis le fichier sauvegardé\n",
    "# Chargement base de travail\n",
    "#with open(os.path.join(path_travail, 'df_dumN.pkl'), 'rb') as f:\n",
    "#    data = pkl.load(f)  \n",
    "#X, Y = data['features'], data['labels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On rappelle que la modélisation se fait en trois temps :\n",
    "- on sépare les données : TRAIN / TEST\n",
    "- on apprend le modèle sur TRAIN\n",
    "- on évalue la performance du modèle appris sur TEST\n",
    "\n",
    "On coupe les données de façon aléatoire en deux groupes. \n",
    "Le plus souvent, on les sépare  en 80% pour l'apprentissage et 20% pour le test. D'autres pourcentages courants sont 67-33 ou 50-50.\n",
    "On   utilise la  fonction `train_test_split` du package `sklearn.model_selection` pour séparer aléatoirement les données. \n",
    "\n",
    "Même si le split est aléatoire, on souhaite que les deux échantillons soient tous les deux représentatifs du problème. En particulier, on voudrait qu'ils contiennent le même pourcentage de labels 0 et 1, ce qui est notamment important quand  les labels ne sont pas équilibrés (pas 50-50).\n",
    "\n",
    "Ici, le taux de défaut, i.e. la proportion de clients qui n'ont pas remboursé leur crédit (y=1) dans la variable à prédire `Y` dans l'échantillon global est :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculons le taux de cible\n",
    "tx_cible = Y.mean()\n",
    "# Affichons le taux de cible joliment\n",
    "print(\"Taux de cible: {taux:.2f}%\".format(taux=100 * tx_cible))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux de défaut est bas, donc une coupe totalement aléatoire des données risque de produire des sous-échantillons où l'un des deux ne contient que  peu de labels qui valent 1. Cela peut être problématique pour l'apprentissage du modèle comme pour l'évaluation de la méthode.\n",
    "En pratique, dans des problèmes de classification, on force alors la même répartition des labels dans les deux échantillons TRAIN et TEST, via l'option `stratify`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercice 3\n",
    "\n",
    "Utiliser la  fonction `train_test_split` du package `sklearn.model_selection` pour séparer les données en `train` et `test` avec 80% pour les données `train` et 20% pour les données `test`. On force la même répartition des labels dans les deux sous-échantillons.\n",
    "**Pour ce TP** fixons la graine du générateur aléatoire (via l'option `random_state=19`) afin que nous travaillons tous sur les mêmes données `train` et `test`. En effet,  la sélection de variables par pénalisation $\\ell_1$ est très instable et dépend énormément du split des données effectué. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split TRAIN / TEST\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, stratify=Y, test_size=0.2, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(Y_train.mean(), Y_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les échantillons  `train` et `test` ont exactement le même taux de défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Apprentissage par régression logistique \n",
    "\n",
    "Pour commencer, on va apprendre, sur les données `train`, les coefficients de la régression logistique de `Y` sur l'ensemble des variables `X` (soit 32 variables). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4\n",
    "- Utiliser la fonction `LogisticRegression` de `sklearn.linear_model` pour effectuer une régression logistique simple (i.e. sans pénalisation). Visualiser les coefficients estimés (intercept et variables).\n",
    "- Puis sur le jeu `test`, obtenez les probabilités de défaut de chaque individu (`predict_proba()`) et les prédictions (`predict()`) pour le seuil par défaut $t=1/2$. Utiliser la fonction `score` pour calculer le pourcentage de bien classés (`accuracy`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Définition du modèle de régression logistique : sans pénalisation \n",
    "logreg = LogisticRegression(penalty='none', max_iter=1000) # sans l'option max_iter, on peut avoir un nb max d'itérations insuffisant pour atteindre la convergence\n",
    "# On estime les paramètres de ce modèle sur les données \n",
    "result = logreg.fit(X_train, Y_train)\n",
    "\n",
    "# Résultats de la regression logistique\n",
    "# On visualise l'intercept b et les 32 coefficients w_i estimés pour les 32 variables \n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Probabilités de non-défaut (y=0) et de défaut (y=1) pour chaque individu dans le jeu test \n",
    "result.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On obtient deux colonnes qui donnent les probas prédites pour les deux labels 0 et 1.\n",
    "On peut constater que pour certains individus, la prédiction est très tranchée (93% vs 7%) tandis que pour d'autres, la prédiction est beaucoup plus incertaine (52% vs 48%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Prédiction du label sur nos données test - ici on applique le seuil t=1/2 sur les probas ci-dessus\n",
    "Y_pred =result.predict(X_test) \n",
    "print(Y_pred)\n",
    "print(Y_pred.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On constate qu'on prédit seulement 4,5% de défaut, alors que les jeux de données ont été stratifiés, donc on sait qu'il y a 15% de défaut dans les données TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy (i.e. pourcentage de bien classés) sur les données de test\n",
    "result.score(X_test,Y_test)\n",
    "\n",
    "## Autre façon d'obtenir la même chose :\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "#accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "L'accuracy est bonne alors qu'on a peu de défaut prédit : c'est parce que les 0 sont très nombreux ! Un prédicteur constant à 0 donne une accuracy de 85% !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Matrice de confusion \n",
    "from sklearn import metrics\n",
    "\n",
    "metrics.confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Courbes ROC et precision-recall\n",
    "À présent on veut regarder les performances du classifieur en faisant jouer des rôles non symétriques aux valeurs 0 et 1. En effet pour une banque, il est plus important d'identifier les vrais négatifs ($y=0$ et $\\hat y=0$ i.e. les clients à qui on va octroyer un crédit et qui vont effectivement le rembourser) alors qu'on peut se permettre d'avoir des faux positifs ($y=0$ et $\\hat y =1$ i.e. des clients à qui on n'accorde pas le crédit alors qu'ils l'auraient remboursé). \n",
    "\n",
    "Noter que du point de vue du client, les priorités ne sont pas les mêmes ! De même si on s'intéressait à un organisme social de micro-crédit, on voudrait au contraire limiter les faux positifs. Suivant le problème considéré toutes ces quantités (TP, FP, TN, FN) n'ont pas la même importance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercice 5\n",
    "\n",
    "- Tracer les courbes ROC et precision-recall obtenues sur le jeu `test`. \n",
    "- À titre de comparaison, on tracera aussi les mêmes courbes obtenues sur le jeu de données `train`. On rappelle que les performances du classifieur doivent toujours être calculées à partir d'un échantillon `test` indépendant du jeu de données `train` qui a servi à la construction du classifieur. Cependant il est utile de comparer les courbes ROC calculées avec la base `test` versus la base `train` pour détecter un sur-apprentissage potentiel. \n",
    "- On utilisera la librairie `matplotlib.pyplot` pour tracer les graphiques et les fonctions `roc_curve` et `auc` de `sklearn.metrics` pour calculer les points de la courbe ROC et l'AUC.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "# COURBES ROC ET AUC \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "# Sur le jeu de TEST, on reprend les scores prédits (probas que Y=1) \n",
    "scores_test= logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, label=f\"Test - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "print('AUC sur test=%.2f' %metrics.auc(fpr, tpr))\n",
    "\n",
    "# La même courbe mais sur le jeu TRAIN\n",
    "scores_train= logreg.predict_proba(X_train)[:,1]\n",
    "fpr, tpr, _ = roc_curve(Y_train,scores_train)\n",
    "ax.plot(fpr, tpr, label=f\"Train - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "print('AUC sur train=%.2f' %metrics.auc(fpr, tpr))\n",
    "\n",
    "fig.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Variante pour AUC-ROC\n",
    "from sklearn.metrics import roc_auc_score \n",
    "\n",
    "AUC_test = roc_auc_score(Y_test, scores_test)\n",
    "print('ROC_AUC_Score sur test =%.2f' %AUC_test)\n",
    "AUC_train = roc_auc_score(Y_train, scores_train)\n",
    "print('ROC_AUC_Score sur train=%.2f' %AUC_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# COURBES PRECISION-RECALL\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [1,0], 'k--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "\n",
    "# Sur le jeu de TEST :\n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test)\n",
    "# Attention le recall est en abscisses et la précision en ordonnée\n",
    "ax.plot(recall, precision, label=f\"Test - AUC = %0.2f\" % metrics.auc(recall,precision)) \n",
    "# On calcule l'aire sous la courbe precision-recall exactement comme pour la courbe ROC\n",
    "\n",
    "# La même courbe mais sur le jeu TRAIN\n",
    "precision, recall, _ = precision_recall_curve(Y_train,scores_train)\n",
    "ax.plot(recall, precision, label=f\"Train - AUC = %0.2f\" % metrics.auc(recall,precision))\n",
    "\n",
    "fig.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**Remarques** : \n",
    "* les courbes ROC et Precision-Recall ne se lisent pas de la même façon.\n",
    "* Ici on a une AUC-PR très mauvaise. En effet, le classifieur a tendance à prédire beaucoup de 0 et les classes sont non équilibrées. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression logistique avec pénalité Ridge\n",
    "\n",
    "Dans cette partie, on va utiliser une pénalité en norme 2 (dite $\\ell_2$ ou Ridge) sur les coefficients de la régression. Cette pénalité Ridge a tendance à \"rétrecir\" les valeurs des coefficients (on parle de shrinkage) et régularise la solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Exercice 6\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_2$ sur les données d'apprentissage. On utilisera toujours la fonction `LogisticRegression`avec  l'option `penalty='l2'`. En plus, on utilisera une pénalité assez forte, p. ex. `C=0.01`. \n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité (visualisez l'effet de shrinkage). \n",
    "- Évaluer les performances du classifieur sur les données de test.\n",
    "- Pour les deux classifieurs (logreg et sa version pénalisée en norme 2), comparer les courbes de score sur les données `test` ainsi que les performances en termes de courbes (ROC ou precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définition du modèle de régression logistique : pénalisation norme 2 \n",
    "logreg_l2 = LogisticRegression(penalty='l2', C=0.01, max_iter=1000)\n",
    "# On estime les paramètres de ce modèle sur les données \n",
    "result_l2 = logreg_l2.fit(X_train, Y_train)\n",
    "\n",
    "# Resultats de la regression logistique\n",
    "# On visualise l'intercept b et les 32 coefficients w_i estimés pour les 32 variables - On les compare aux précédents\n",
    "print(result_l2.intercept_)\n",
    "print(result.intercept_ )\n",
    "print('------')\n",
    "print(result_l2.coef_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation des différences en valeur absolue\n",
    "\n",
    "# Declaring the figure (width, height)\n",
    "plt.figure(figsize=[15, 10])\n",
    "\n",
    "# Data to be plotted\n",
    "height = np.abs(np.concatenate(result.coef_)) # np.concatenate transforme un array en vecteur\n",
    "height_l2 = np.abs(np.concatenate(result_l2.coef_))\n",
    "variables = X.columns\n",
    "x_pos = np.arange(len(variables))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x_pos, height, color = 'b', width = 0.25)\n",
    "plt.bar(x_pos+ 0.25, height_l2, color = 'g', width = 0.25)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos,variables)\n",
    "\n",
    "# Creating the legend of the bars in the plot\n",
    "plt.legend(['No penalty', 'norm-2 penalty'])\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On constate l'effet de *shrinkage*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On trace les courbes de score sur les données de test pour nos deux classifieurs\n",
    "pred_test_l2 = pd.DataFrame({\n",
    "    'Y': Y_test, \n",
    "    'score' : result_l2.predict_proba(X_test)[:,1], \n",
    "    })\n",
    "\n",
    "pred_test = pd.DataFrame({\n",
    "    'Y': Y_test, \n",
    "    'score' : result.predict_proba(X_test)[:,1], \n",
    "    })\n",
    "\n",
    "# on ordonne les prédictions par score décroissant\n",
    "pred_test_l2 = pred_test_l2.sort_values(by='score', ascending=False) \n",
    "pred_test = pred_test.sort_values(by='score', ascending=False) \n",
    "\n",
    "plt.xlabel('individuals (score decreasing order)')\n",
    "plt.ylabel('score')\n",
    "\n",
    "# Courbes de scores \n",
    "plt.subplot()\n",
    "x = np.linspace(0, 1, len(pred_test.score))\n",
    "\n",
    "plt.plot(x, pred_test_l2.score, linewidth=2, color='r', label=f\"LogReg-Ridge\") \n",
    "plt.plot(x, pred_test.score, linewidth=2, color='b', label=f\"LogReg (no penalty)\") \n",
    "\n",
    "# repères \n",
    "plt.vlines(0.15,0,1,color='gray',linestyle='dashed') # 15% d'individus avec y=1\n",
    "plt.hlines(0.5,0,1,color='gray',linestyle='dashed') # seuil t=1/2\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ici on constate que les deux prédicteurs sont différents. La pénalisation donne des probabilités moins tranchées pour les deux classes, c'est-à-dire les scores élevés (resp. faibles) sont plus faibles (resp. plus forts) que pour la régression non pénalisée. Les 15% d'individus avec $y=1$ devraient être à gauche de la ligne grise verticale (score de défaut élevé) et la régression pénalisée $\\ell_2$ leur attribue un score inférieur à celui donné par la régression simple. La droite grise horizontale correspond au seuil $t=0.5$: dans une approches naive on prédit 1 si la probabilité calculée par le modèle (le score) dépasse 0.5.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Comparons les deux classifieurs via leurs courbes ROC et les AUC-ROC\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "# Sur le jeu de TEST, on reprend les scores prédits (probas que Y=1) \n",
    "scores_test_l2 = logreg_l2.predict_proba(X_test)[:,1]\n",
    "Y_pred_l2= logreg_l2.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test_l2) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, label=f\"LogReg-Ridge - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "scores_test= logreg.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, label=f\"LogReg (no penalty) - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Les performances sont les mêmes en termes d'AUC-ROC. Regardons ce qui se passe pour la courbe precision-recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On compare maintenant les deux classifieurs via leurs courbes PR et les valeurs AUC-PR\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [1,0], 'k--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "\n",
    "# Sur le jeu de TEST \n",
    "# pour la prediction pénalisée en norme 2\n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test_l2)\n",
    "ax.plot(recall, precision, label=f\"LogReg-Ridge - AUC = %0.2f\" % metrics.auc(recall,precision)) \n",
    "\n",
    "#  pour la prédiction sans pénalité \n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test) \n",
    "ax.plot(recall, precision, label=f\"LogReg (no penalty) - AUC = %0.2f\" % metrics.auc(recall,precision))\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Là encore, on a les mêmes performances en termes d'AUC-PR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression logistique avec sélection de variables\n",
    "\n",
    "Dans cette partie, on va mettre en oeuvre une pénalité $\\ell_1$ dans la régression logistique. Cela va nous permettre de faire de la sélection de variables : les variables peu informatives pour la prédiction ne seront plus du tout utilisées (coefficient $w_i$ estimé à 0). On espère ainsi améliorer les performances de notre prédicteur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 7\n",
    "\n",
    "- Mettre en oeuvre la régression logistique avec pénalité $\\ell_1$ sur les données d'apprentissage (avec l'option le `solver='liblinear'`). On utilisera une pénalité assez forte (ex $C=0.01$). \n",
    "- Comparer les valeurs des coefficients avec le cas sans pénalité (visualisez l'effet d'annulation des coefficients). \n",
    "- Évaluer les performances du classifieur sur les données `test`.\n",
    "- Pour les deux classifieurs (logreg et sa version pénalisée en norme 1), comparer les courbes de score sur les données `test` ainsi que les performances en termes de courbes (ROC et precision-recall) et d'AUC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Définition du modèle de régression logistique : pénalisation norme 1 \n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.01, max_iter=1000)\n",
    "# On estime les paramètres de ce modèle sur les données \n",
    "result_l1 = logreg_l1.fit(X_train, Y_train)\n",
    "\n",
    "# Resultats de la regression logistique pénalisée en norme 1\n",
    "# On visualise l'intercept b et les 32 coefficients w_i estimés pour les 32 variables \n",
    "print(result_l1.intercept_)\n",
    "print('------')\n",
    "print(result_l1.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On constate qu'un grand nombre de coefficients de la régression pénalisée $\\ell_1$ sont mis à 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation des différences en valeur absolue\n",
    "\n",
    "# Initialiser la figure [width, height]\n",
    "plt.figure(figsize=[15, 10])\n",
    "\n",
    "# Data to be plotted\n",
    "height = np.abs(np.concatenate(result.coef_)) # np.concatenate transforme un array en vecteur\n",
    "height_l2 = np.abs(np.concatenate(result_l2.coef_))\n",
    "height_l1 = np.abs(np.concatenate(result_l1.coef_))\n",
    "variables = X.columns\n",
    "x_pos = np.arange(len(variables))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x_pos, height, color = 'b', width = 0.25)\n",
    "plt.bar(x_pos+ 0.25, height_l2, color = 'g', width = 0.25)\n",
    "plt.bar(x_pos+ 0.5, height_l1, color = 'red', width = 0.25)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos,variables)\n",
    "\n",
    "# Creating the legend of the bars in the plot\n",
    "plt.legend(['No penalty', 'norm-2 penalty','norm-1 penalty'])\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On trace les courbes de score sur les données de test pour nos deux classifieurs (sans pénalité et pénalité en norme 1)\n",
    "\n",
    "pred_test_l1 = pd.DataFrame({\n",
    "    'Y': Y_test, \n",
    "    'score' : result_l1.predict_proba(X_test)[:,1], \n",
    "    })\n",
    "\n",
    "# on ordonne les prédictions par score décroissant\n",
    "pred_test_l1 = pred_test_l1.sort_values(by='score', ascending=False) \n",
    "\n",
    "plt.xlabel('individuals (score decreasing order)')\n",
    "plt.ylabel('score')\n",
    "\n",
    "# Courbes de scores \n",
    "plt.subplot()\n",
    "x = np.linspace(0, 1, len(pred_test.score))\n",
    "\n",
    "plt.plot(x, pred_test_l1.score, linewidth=2, color='r', label=f\"LogReg-L1-penalty\") \n",
    "plt.plot(x, pred_test_l2.score, linewidth=2, color='green', label=f\"LogReg-L2-penalty\") \n",
    "plt.plot(x, pred_test.score, linewidth=2, color='b', label=f\"LogReg (no penalty)\") \n",
    "\n",
    "# repères \n",
    "plt.vlines(0.15,0,1,color='gray',linestyle='dashed') # 15% d'individus avec y=1\n",
    "plt.hlines(0.5,0,1,color='gray',linestyle='dashed') # seuil t=1/2\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Encore une fois, on constate que les deux prédicteurs sont différents. La pénalisation donne des probabilités moins tranchées pour les deux classes, i.e.  les scores élevés (resp. faibles) sont plus faibles (resp. plus forts) que pour la régression non pénalisée. Les 15% d'individus avec $y=1$ devraient être à gauche (score de défaut élevé) et la régression pénalisée $\\ell_1$ leur attribue un score inférieur à celui donné par la régression simple ou par la régression $\\ell_2$.  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Comparons les classifieurs via leurs courbes ROC et les AUC-ROC\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "# Sur le jeu de TEST, on reprend les scores prédits (probas que Y=1) \n",
    "scores_test_l1= logreg_l1.predict_proba(X_test)[:,1]\n",
    "Y_pred_l1= logreg_l1.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test_l1) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, color='r',label=f\"LogReg-L1-penalty - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "# courbe ROC régression simple\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, color='b',label=f\"LogReg (no penalty) - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "# courbe ROC régression pénalisée L2\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test_l2) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, color='green',label=f\"LogReg (no penalty) - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Les performances sont légèrement moins bonnes en termes d'AUC-ROC. Regardons ce qui se passe pour la courbe precision-recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On compare maintenant les deux classifieurs via leurs courbes PR et les valeurs AUC-PR\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [1,0], 'k--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "\n",
    "# Sur le jeu de TEST \n",
    "# pour la prediction pénalisée en norme 1\n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test_l1)\n",
    "ax.plot(recall, precision, color='r',label=f\"LogReg-L1-penalty - AUC = %0.2f\" % metrics.auc(recall,precision)) \n",
    "\n",
    "#  pour la prédiction sans pénalité \n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test) \n",
    "ax.plot(recall, precision, color='b',label=f\"LogReg (no penalty) - AUC = %0.2f\" % metrics.auc(recall,precision))\n",
    "\n",
    "# pour la prediction pénalisée en norme 2\n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test_l2)\n",
    "ax.plot(recall, precision, color='green',label=f\"LogReg-L2-penalty - AUC = %0.2f\" % metrics.auc(recall,precision)) \n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Là encore, on a des performances un peu dégradées pour la régression en pénalité $\\ell_1$ en termes d'AUC-PR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chemins de régularisation pour la pénalité $\\ell_1$\n",
    "\n",
    "Dans cette partie, on va visualiser l'effet de la **constante de pénalité** sur l'évolution des coefficients $w_i$, à travers les **chemins de régularisation**. Lorsque la pénalité est très forte (i.e $C$ très petite), aucune  variable n'est sélectionnée (tous les coefficients $w_i$ sont estimés à 0). Dans ce cas, la fonction de régression logistique est constante (on a seulement l'intercept) et le lien entre $y$ et $X$ est très mal appris. Puis au fur et à mesure que $C$ augmente, on inclue de plus en plus de variables dans notre modèle de régression logistique. Lorsque $C$ est très grande, on ne pénalise plus et on retrouve les résultats de la régression logistique simple. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 8\n",
    "\n",
    "Fixer une grille de valeurs pour la constante $C$, fitter le modèle avec ces constantes et visualiser  les chemins de régularisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On fabrique un ensemble de valeurs C entre 10^(-4) et 10^2\n",
    "c_path = np.logspace(-4, 2, 10)\n",
    "# c_path plus long ==> durée calcul plus longue\n",
    "print(c_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Chemin de régularisation \n",
    "logregC = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "coeffs = []\n",
    "for c in c_path:\n",
    "    logregC.C = c\n",
    "    logregC.fit(X_train, Y_train)\n",
    "    coeffs.append(logregC.coef_.ravel().copy())\n",
    "\n",
    "coeffs = np.array(coeffs)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.semilogx(c_path, coeffs)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.xlabel('C', fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel('Coefficients', fontsize=16)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.title('Evolution de chaque coef de la régression logistique '\n",
    "          'avec la penalisation L1', fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation croisée pour le choix de la constante de pénalité \n",
    "\n",
    "### Régression logistique avec sélection de variables (pénalité $\\ell_1$ + validation croisée)\n",
    "\n",
    "À chaque fois qu'on utilise une pénalité, il faut choisir la constante de pénalisation. \n",
    "Attention : la fonction `LogisticRegression` ne sait pas choisir la constante de pénalisation, qui par défaut est fixée à 1. Ce choix est parfaitement débile (car il ne correspond à rien) et n'a aucune raison d'être utilisé. \n",
    "\n",
    "Dans cette partie, on va mettre en oeuvre le choix de la constante de pénalisation par **validation croisée**. \n",
    "Nous le ferons dans le cadre d'une pénalité $\\ell_1$, mais on pourrait faire exactement la même chose avec un autre type de pénalité. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 9\n",
    "\n",
    "Utiliser la fonction `LogisticRegressionCV` pour sélectionner automatiquement par validation croisée  la constante $C$ de pénalité dans un modèle de régression logistique avec pénalité $\\ell_1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# Modèle de régression logistique avec pénalité l1 et validation croisée pour le choix automatique de la constante de pénalité C\n",
    "logreg_cv = LogisticRegressionCV(penalty='l1',\n",
    "                                 tol=1e-3,\n",
    "                                 Cs=np.logspace(-3, 5, 15), # grille de 15 valeurs pour C entre 10^-3 et 10^5\n",
    "                                 cv=3, # normalement CV avec 10 folds ; ici 3 pour gagner du temps\n",
    "                                 solver='liblinear',\n",
    "                                 # class_weight='balanced',\n",
    "                                 scoring='roc_auc') # le critère pour comparer les différents modèles par CV - Par défaut c'est l'accuracy\n",
    "logreg_cv.fit(X_train, Y_train)\n",
    "\n",
    "# résultats intermédiaires de calculs de ROC-AUC sur chacun des cv-folds\n",
    "crit = logreg_cv.scores_[1]\n",
    "print(crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "np.logspace(-3, 5, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation du critère sur chaque fold pour les différentes valeurs de C\n",
    "\n",
    "# Declaring the figure (width, height)\n",
    "plt.figure(figsize=[15, 10])\n",
    "\n",
    "# X-axis\n",
    "variables = np.round(np.logspace(-3, 5, 15),2) # on arrondit les valeurs de C affichées\n",
    "x_pos = np.arange(len(variables))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x_pos, crit[0], color = 'b', width = 0.25)\n",
    "plt.bar(x_pos+ 0.25, crit[1], color = 'g', width = 0.25)\n",
    "plt.bar(x_pos+ 0.55, crit[2], color = 'r', width = 0.25)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos,variables)\n",
    "\n",
    "# Creating the legend of the bars in the plot\n",
    "plt.legend(['ROC-AUC-1', 'ROC-AUC-2','ROC-AUC-3'])\n",
    "\n",
    "# Show graphic\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Ci-dessus pour chacun des folds, on a la valeur du critère (ROC_AUC) pour chacune des 15 valeurs différentes de la constante C. La variabilité sur chacune des 3 répétitions est due à l'aléa. Lorsque $C$ est très petite, on pénalise beaucoup et le critère ROC-AUC n'est pas bon. Lorsque $C$ augmente, à partir d'un moment on n'améliore pas la valeur du critère. \n",
    "\n",
    "Ce qui nous intéresse, pour chaque valeur de C, c'est la valeur du critère moyennée sur les cv répétitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualisation du critère global de CV \n",
    "\n",
    "score_boot = crit.mean(axis=0)\n",
    "print(score_boot)\n",
    "\n",
    "# Declaring the figure (width, height)\n",
    "plt.figure(figsize=[15, 10])\n",
    "\n",
    "# X-axis\n",
    "#variables = np.logspace(-3, 5, 15)\n",
    "#x_pos = np.arange(len(variables))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(x_pos, score_boot, color = 'b', width = 0.25)\n",
    "# Create names on the x-axis and title\n",
    "plt.xticks(x_pos,variables)\n",
    "plt.title('ROC-AUC-CV', fontsize=18)\n",
    "\n",
    "# Show graphic\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "On voit bien que pour C=10^(-3), la pénalité est trop forte. On ne sélectionne aucune variable et on prédit au hasard (AUC-ROC=0,5).\n",
    "\n",
    "Quand C devient grand, la pénalité diminue, et on sur-ajuste le modèle. \n",
    "\n",
    "La valeur optimale (qui donne le plus grand ROC_AUC moyen) est obtenue directement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "C_opt = logreg_cv.C_[0]\n",
    "print(\"C optimale =\", C_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 10\n",
    "\n",
    "Il faut à présent apprendre le modèle (de régression logistique pénalisée en nomre 1) avec ce C optimal (car pour l'instant on a 3 classifieurs différents construits avec cette valeur sur chacun des folds et de toutes façons on y a pas accès !). Ensuite, tracer les courbes ROC et PR et les comparer au classifieur sans pénalité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Réponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Régression logistique avec pénalisation norme 1 et constante optimale (choisie par CV)\n",
    "logreg_l1Coptim = LogisticRegression(penalty='l1', solver='liblinear', C=C_opt, max_iter=1000)\n",
    "# On estime les paramètres de ce modèle sur les données \n",
    "result_l1Coptim = logreg_l1Coptim.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Comparons le modèle complet sans pénalité avec le modèle pénalisé avec sélection automatique de la constante de pénalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Comparons les classifieurs via leurs courbes ROC et les AUC-ROC\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "\n",
    "# Sur le jeu de TEST, on reprend les scores prédits (probas que Y=1) \n",
    "scores_test_l1Coptim= logreg_l1Coptim.predict_proba(X_test)[:,1]\n",
    "Y_pred_l1Coptim= logreg_l1Coptim.predict(X_test)\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test_l1Coptim) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, color='r',label=f\"LogReg-L1-penalty-Copt - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "# courbe ROC régression simple\n",
    "fpr, tpr, _ = roc_curve(Y_test, scores_test) # on calcule les FPR et TPR pour tout un ensemble de seuils t non précisés\n",
    "ax.plot(fpr, tpr, color='b',label=f\"LogReg (no penalty) - AUC = %0.2f\" % auc(fpr, tpr))\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Les performances semblent identiques, mais la version pénalisée utilise beaucoup moins de variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# On compare maintenant les deux classifieurs via leurs courbes PR et les valeurs AUC-PR\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([0, 1], [1,0], 'k--')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "\n",
    "# Sur le jeu de TEST \n",
    "# pour la prediction pénalisée en norme 1\n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test_l1Coptim)\n",
    "ax.plot(recall, precision, color='r',label=f\"LogReg-L1-penalty-Coptim - AUC = %0.2f\" % metrics.auc(recall,precision)) \n",
    "\n",
    "#  pour la prédiction sans pénalité \n",
    "precision, recall, _ = precision_recall_curve(Y_test, scores_test) \n",
    "ax.plot(recall, precision, color='b',label=f\"LogReg (no penalty) - AUC = %0.2f\" % metrics.auc(recall,precision))\n",
    "\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aller plus loin\n",
    "\n",
    "On peut passer un temps infini à trouver le meilleur modèle. Globalement, on peut travailler sur deux parties : \n",
    "1. les variables en entrée : garder tous les features ? aussi ceux qui sont très corrélés ? et pour les variables qualitatives, garder tous les dummies ? ou regrouper des catégories de faible taille ?\n",
    "2. trouver les meilleurs hyperparamètres des modèles, en particulier par validation croisée excessive.\n",
    "\n",
    "A vous de jouer, si vous avez encore un peu d'énergie :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
